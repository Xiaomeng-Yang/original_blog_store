<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>STR Papers List</title>
    <url>/2022/07/11/STR-papers-list/</url>
    <content><![CDATA[<p>This post lists papers about scene text recognition published in top conferences.</p>
<h1 id="Latest-Papers-updated-on-11-20"><a href="#Latest-Papers-updated-on-11-20" class="headerlink" title="Latest Papers (updated on 11.20)"></a>Latest Papers (updated on 11.20)</h1><ul>
<li>Arxiv: [IterVM: Iterative Vision Modeling Module for Scene Text Recognition]</li>
<li>Applied intelligence: [Scene text recognition based on two-stage attention and multi-branch feature fusion module]</li>
<li>ICPR-2022: [Portmanteauing Features for Scene Text Recognition]</li>
<li>TMM-2022: [Dual Relation Network for Scene Text Recognition]</li>
</ul>
<h1 id="Arxiv"><a href="#Arxiv" class="headerlink" title="Arxiv"></a>Arxiv</h1><ul>
<li><a href="https://arxiv.org/pdf/2206.00311v1.pdf">MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining</a> [2022/6/1]</li>
<li><a href="https://arxiv.org/pdf/2112.00948.pdf">Visual-Semantic Transformer for Scene Text Recognition</a> [2021/12/2]</li>
<li><a href="https://arxiv.org/pdf/2111.15263v1.pdf">Multi-modal Text Recognition Networks: Interactive Enhancements between Visual and Semantic Features</a> [2021/11/30]</li>
<li>[Multimodal Semi-Supervised Learning for Text Recognition]</li>
<li>[Towards Open-Set Text Recognition via Label-to-Prototype Learning]</li>
<li>[Scene Text Recognition with Single-Point Decoding Network]</li>
</ul>
<h1 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h1><ul>
<li>[2022-TIP] [PETR: Rethinking the Capability of Transformer-Based Language Model in Scene Text Recognition]</li>
<li>[ACCESS-2022] [Scene Text Recognition with Semantics]</li>
<li>[BMCV-2022] [Masked Vision-Language Transformers for Scene Text Recognition]</li>
</ul>
<a id="more"></a>
<h1 id="Sort-by-Conferences-The-latest-three-years"><a href="#Sort-by-Conferences-The-latest-three-years" class="headerlink" title="Sort by Conferences (The latest three years)"></a>Sort by Conferences (The latest three years)</h1><h2 id="2022-ECCV"><a href="#2022-ECCV" class="headerlink" title="2022 ECCV"></a>2022 ECCV</h2><ul>
<li><a href>SGBANet: Semantic GAN and Balanced Attention Network for Arbitrarily Oriented Scene Text Recognition</a></li>
<li><a href>Scene Text Recognition with Permuted Autoregressive Sequence Models</a></li>
<li>[Levenshtein OCR]</li>
<li>[Toward Understanding WordArt: Corner-Guided Transformer for Scene Text Recognition]</li>
<li>[Multi-Granularity Prediction for Scene Text Recognition]</li>
<li>[Vision-Language Adaptive Mutual Decoder for OOV-STR]</li>
</ul>
<h2 id="2022-MM"><a href="#2022-MM" class="headerlink" title="2022 MM"></a>2022 MM</h2><ul>
<li><a href="https://arxiv.org/pdf/2207.00193.pdf">Reading and Writing: Discriminative and Generative Modeling for Self-Supervised Text Recognition</a></li>
</ul>
<h2 id="2022-IJCAI"><a href="#2022-IJCAI" class="headerlink" title="2022 IJCAI"></a>2022 IJCAI</h2><ul>
<li><a href="https://arxiv.org/pdf/2205.00159.pdf">SVTR: Scene Text Recognition with a Single Visual Model</a></li>
</ul>
<h2 id="2022-AAAI"><a href="#2022-AAAI" class="headerlink" title="2022 AAAI"></a>2022 AAAI</h2><ul>
<li><a href="https://federated-learning.org/fl-aaai-2022/Papers/FL-AAAI-22_paper_6.pdf">FedOCR: Efficient and Secure Federated Learning for Scene Text Recognition</a></li>
<li><a href="https://www.aaai.org/AAAI22Papers/AAAI-10147.ZhangX.pdf">Context-based Contrastive Learning for Scene Text Recognition</a></li>
<li><a href="https://www.aaai.org/AAAI22Papers/AAAI-1905.HeY.pdf">Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition</a></li>
<li><a href="https://www.aaai.org/AAAI22Papers/AAAI-785.LiuH.pdf">Perceiving Stroke-Semantic Context: Hierarchical Contrastive Learning for Robust Scene Text Recognition</a></li>
</ul>
<h2 id="2022-CVPR"><a href="#2022-CVPR" class="headerlink" title="2022 CVPR"></a>2022 CVPR</h2><ul>
<li><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Pushing_the_Performance_Limit_of_Scene_Text_Recognizer_Without_Human_CVPR_2022_paper.pdf">Pushing the Performance Limit of Scene Text Recognizer without Human Annotation</a></li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Open-Set_Text_Recognition_via_Character-Context_Decoupling_CVPR_2022_paper.pdf">Open-set Text Recognition via Character-Context Decoupling</a></li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_SimAN_Exploring_Self-Supervised_Representation_Learning_of_Scene_Text_via_Similarity-Aware_CVPR_2022_paper.pdf">SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization</a></li>
</ul>
<h2 id="2021-ICCV"><a href="#2021-ICCV" class="headerlink" title="2021 ICCV"></a>2021 ICCV</h2><ul>
<li><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Bhunia_Towards_the_Unseen_Iterative_Text_Recognition_by_Distilling_From_Errors_ICCV_2021_paper.pdf">Towards the Unseen: Iterative Text Recognition by Distilling from Errors</a></li>
<li><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Bhunia_Joint_Visual_Semantic_Reasoning_Multi-Stage_Decoder_for_Text_Recognition_ICCV_2021_paper.pdf">Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition</a></li>
<li><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Bhunia_Text_Is_Text_No_Matter_What_Unifying_Text_Recognition_Using_ICCV_2021_paper.pdf">Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation</a></li>
<li><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_From_Two_to_One_A_New_Scene_Text_Recognizer_With_ICCV_2021_paper.pdf">From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network</a></li>
</ul>
<h2 id="2021-CPVR"><a href="#2021-CPVR" class="headerlink" title="2021 CPVR"></a>2021 CPVR</h2><ul>
<li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Read_Like_Humans_Autonomous_Bidirectional_and_Iterative_Language_Modeling_for_CVPR_2021_paper.pdf">Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition</a></li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Nguyen_Dictionary-Guided_Scene_Text_Recognition_CVPR_2021_paper.pdf">Dictionary-guided Scene Text Recognition</a></li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Primitive_Representation_Learning_for_Scene_Text_Recognition_CVPR_2021_paper.pdf">Primitive Representation Learning for Scene Text Recognition</a></li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Aberdam_Sequence-to-Sequence_Contrastive_Learning_for_Text_Recognition_CVPR_2021_paper.pdf">Sequence-to-Sequence Contrastive Learning for Text Recognition</a></li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Baek_What_if_We_Only_Use_Real_Datasets_for_Scene_Text_CVPR_2021_paper.pdf">What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels</a></li>
</ul>
<h2 id="2021-AAAI"><a href="#2021-AAAI" class="headerlink" title="2021 AAAI"></a>2021 AAAI</h2><ul>
<li><a href="https://arxiv.org/pdf/2005.13117.pdf">SPIN：Structure-Preserving Inner Offset Network for Scene Text Recognition</a></li>
</ul>
<h2 id="2021-MM"><a href="#2021-MM" class="headerlink" title="2021 MM"></a>2021 MM</h2><ul>
<li><a href="https://dl.acm.org/doi/10.1145/3474085.3475238">PIMNet: A Parallel, Iterative and Mimicking Network for Scene Text Recognition</a></li>
</ul>
<h2 id="2020-ECCV"><a href="#2020-ECCV" class="headerlink" title="2020 ECCV"></a>2020 ECCV</h2><ul>
<li><a href="https://arxiv.org/pdf/2007.07542.pdf">Robustscanner: Dynamically Enhancing Positional Clues for Robust Text Recognition</a></li>
</ul>
<h2 id="2020-CVPR"><a href="#2020-CVPR" class="headerlink" title="2020 CVPR"></a>2020 CVPR</h2><ul>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Litman_SCATTER_Selective_Context_Attentional_Scene_Text_Recognizer_CVPR_2020_paper.pdf">SCATTER: Selective Context Attentional Scene Text Recognizer</a></li>
<li><a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w34/Lee_On_Recognizing_Texts_of_Arbitrary_Shapes_With_2D_Self-Attention_CVPRW_2020_paper.pdf">On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention</a></li>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_SEED_Semantics_Enhanced_Encoder-Decoder_Framework_for_Scene_Text_Recognition_CVPR_2020_paper.pdf">Seed: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition</a></li>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Towards_Accurate_Scene_Text_Recognition_With_Semantic_Reasoning_Networks_CVPR_2020_paper.pdf">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks</a></li>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Learn_to_Augment_Joint_Data_Augmentation_and_Network_Optimization_for_CVPR_2020_paper.pdf">Learn to Augment: Joint Data Augmentation and Network Optimization for Text Recognition</a></li>
<li><a href="https://arxiv.org/pdf/2003.10608.pdf">UnrealText: Synthesizing Realistic Scene Text Images from the Unreal World</a></li>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_What_Machines_See_Is_Not_What_They_Get_Fooling_Scene_CVPR_2020_paper.pdf">What Machines See Is Not What They Get: Fooling Scene Text Recognition Models with Adversarial Text Images</a></li>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wan_On_Vocabulary_Reliance_in_Scene_Text_Recognition_CVPR_2020_paper.pdf">On Vocabulary Reliance in Scene Text Recognition</a></li>
</ul>
<h2 id="2020-AAAI"><a href="#2020-AAAI" class="headerlink" title="2020 AAAI"></a>2020 AAAI</h2><ul>
<li><a href="https://arxiv.org/pdf/1912.12422v1.pdf">Textscanner: Reading Characters in Order for Robust Scene Text Recognition</a></li>
<li><a href="https://arxiv.org/pdf/1912.10205.pdf">Decoupled Attention Network for Text Recognition</a></li>
<li><a href="https://arxiv.org/pdf/2002.01276.pdf">GTC: Guided Training of CTC towards Efficient and Accurate Scene Text Recognition</a></li>
</ul>
<h1 id="Sort-by-Methods"><a href="#Sort-by-Methods" class="headerlink" title="Sort by Methods"></a>Sort by Methods</h1><ul>
<li>ESIR: End-to-end scene text recognition via iterative image rectification [2019-CVPR]</li>
<li>Sequence-to-sequence domain adaptation network for robust text image recognition [2019-CVPR]</li>
</ul>
<h2 id="CTC-based-decoder"><a href="#CTC-based-decoder" class="headerlink" title="CTC-based decoder"></a>CTC-based decoder</h2><ul>
<li>SVTR: Scene Text Recognition with a Single Visual Model [IJCAI-2022]<ul>
<li>Transformer encoder + CTC decoder</li>
</ul>
</li>
<li>An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition [TPAMI-2016]<ul>
<li>CRNN Model</li>
</ul>
</li>
<li>STAR-Net: A spatial attention residue network for scene text recognition [BMVC-2016]</li>
<li>GTC: Guided Training of CTC towards Efficient and Accurate Scene Text Recognition [AAAI-2020]<ul>
<li>训练的时候用Attention分支辅助CTC，测试的时候只用CTC</li>
</ul>
</li>
</ul>
<h2 id="Attention-based-decoder"><a href="#Attention-based-decoder" class="headerlink" title="Attention-based decoder"></a>Attention-based decoder</h2><ul>
<li>Towards accurate scene text recognition with semantic reasoning networks [CVPR-2020]</li>
<li>What is wrong with scene text recognition model comparisons? dataset and model analysis [ICCV-2019]<ul>
<li>TRBA, conclusion of previous methods</li>
</ul>
</li>
<li>Show, attend and read: A simple and strong baseline for irregular text recognition [AAAI-2019]<ul>
<li>2D Attention</li>
</ul>
</li>
<li>ASTER: An Attentional Scene Text Recognizer with Flexible Rectification [TPAMI-2018]<ul>
<li>TPS矫正 + encoder + attention decoder</li>
</ul>
</li>
<li>MORAN: A Multi-Object Rectified Attention Network for Scene Text Recognition [PR-2018]<ul>
<li>任意方向矫正</li>
</ul>
</li>
<li>Edit probability for scene text recognition [CVPR-2018]</li>
<li>Focusing attention: Towards accurate text recognition in natural images [ICCV-2017]</li>
<li>Robust Scene Text Recognition With Automatic Rectification [CVPR-2016]<ul>
<li>TPS矫正 + Attention</li>
</ul>
</li>
</ul>
<h2 id="Parallel-attention-decoder"><a href="#Parallel-attention-decoder" class="headerlink" title="Parallel-attention decoder"></a>Parallel-attention decoder</h2><ul>
<li>Towards accurate scene text recognition with semantic reasoning networks<ul>
<li>SRN, see for detail</li>
</ul>
</li>
<li>Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition<ul>
<li>ABINet, see for detail</li>
</ul>
</li>
<li>From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network</li>
<li>Multi-modal Text Recognition Networks: Interactive Enhancements between Visual and Semantic Features</li>
</ul>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul>
<li>Synthetic data and artificial neural net?works for natural scene text recognition [NIPS-2014]<ul>
<li>MJ Dataset</li>
</ul>
</li>
<li>Synthetic data for text localisation in natural images [CVPR-2016]<ul>
<li>SynthText Dataset</li>
</ul>
</li>
<li>UnrealText: Synthesizing realistic scene text images from the unreal world [CVPR-2020]</li>
</ul>
]]></content>
      <categories>
        <category>Scene Text Recognition</category>
      </categories>
      <tags>
        <tag>STR</tag>
      </tags>
  </entry>
  <entry>
    <title>[2022-ECCV] Levenshtein OCR</title>
    <url>/2022/11/29/Levenshtein-OCR/</url>
    <content><![CDATA[<ul>
<li>Paper: <a href="https://arxiv.org/abs/2209.03594v1">https://arxiv.org/abs/2209.03594v1</a></li>
<li>Code: Waiting…</li>
<li>Tips: Iterative refinement, interpretability (whether to depend on the linguistic information or not)</li>
</ul>
<div align="center"> <img src="/2022/11/29/Levenshtein-OCR/overview.jpg" width="600"></div>

<a id="more"></a>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><ul>
<li>Levenshtein Transformer (LevT) in NLP tasks</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><div align="center"> <img src="/2022/11/29/Levenshtein-OCR/architecture.jpg" width="700"></div>

<ul>
<li>Vision-Language Transformer: concatenation</li>
</ul>
<h2 id="Imitation-Learning"><a href="#Imitation-Learning" class="headerlink" title="Imitation Learning"></a>Imitation Learning</h2><p>Refinement task: a Markov Decision Process (MDP) $(\mathcal{y}, \mathcal{A}, \epsilon, \mathcal{R}, y^0)$</p>
<ul>
<li>$y^0 \in \mathcal{y}$: the initial sequence</li>
<li>$\mathcal{A}$: deletion, insertion</li>
<li>$\mathcal{R} = -\mathcal{D}(y, y^*)$: Levenshtein distance</li>
<li>$\epsilon$: trained model</li>
</ul>
<h3 id="Deletion-Action"><a href="#Deletion-Action" class="headerlink" title="Deletion Action"></a>Deletion Action</h3><div align="center"> <img src="/2022/11/29/Levenshtein-OCR/deletion.jpg" width="500"></div>

<h3 id="Insertion-Action"><a href="#Insertion-Action" class="headerlink" title="Insertion Action"></a>Insertion Action</h3><div align="center"> <img src="/2022/11/29/Levenshtein-OCR/placeholder.jpg" width="290"></div>

<p>$W_{plh} \in \mathbb{R}^{M \times 2D}$</p>
<div align="center"> <img src="/2022/11/29/Levenshtein-OCR/inser.jpg" width="500"></div>

<h2 id="Training-Phase"><a href="#Training-Phase" class="headerlink" title="Training Phase"></a>Training Phase</h2><p>Imitation learning: expert policy $\pi^*$ - dynamic programming</p>
<ul>
<li>$a$: deletion, placeholder, insertion</li>
</ul>
<p>Loss function: $L = \lambda_1L_v + \lambda_2L_{del} + \lambda_3L_{ins}$</p>
<h3 id="Deletion"><a href="#Deletion" class="headerlink" title="Deletion"></a>Deletion</h3><div align="center"> <img src="/2022/11/29/Levenshtein-OCR/deletion_loss.jpg" width="300"></div>
<div align="center"> <img src="/2022/11/29/Levenshtein-OCR/deletion_d.jpg" width="300"></div>

<ul>
<li>$y^0$: initial sequence</li>
<li>$\mu &lt; \alpha$: randomly add some symbols on $y^0$ to generate $y_{del}$</li>
<li>$\mu \ge \alpha$: use expert placeholder policy and the learned token prediction to generate $y_{del}$ based on $y’$ (any sequence ready to insert)</li>
</ul>
<h3 id="Insertion"><a href="#Insertion" class="headerlink" title="Insertion"></a>Insertion</h3><div align="center"> <img src="/2022/11/29/Levenshtein-OCR/insertion_loss.jpg" width="300"></div>
<div align="center"> <img src="/2022/11/29/Levenshtein-OCR/insertion_d.jpg" width="300"></div>

<ul>
<li>$\beta &lt; \mu$: randomly delete some symbols on <font color="#32adff">groundtruth</font> $y^*$ to produce $y_{ins}$</li>
<li>$\beta \ge \mu$: expert deletion policy is employed to generate $y_{ins}$ based on the initial sequence $y^0$</li>
</ul>
<h2 id="Inference-Phase"><a href="#Inference-Phase" class="headerlink" title="Inference Phase"></a>Inference Phase</h2><p>Alternatively employ deletion and insertion to refine text at inference process, until two policies converge (<font color="#32adff">either nothing to delete or insert, or reaching maximum iterations</font>).</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><div align="center"> <img src="/2022/11/29/Levenshtein-OCR/table_1.jpg" width="600"></div>

<div align="center"> <img src="/2022/11/29/Levenshtein-OCR/table_2.jpg" width="600"></div>

<div align="center"> <img src="/2022/11/29/Levenshtein-OCR/table_3.jpg" width="600"></div>

<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul>
<li>We propose a novel, <font color="#32adff">cross-modal transformer</font> based scene text recognizer, which fully explores the interactions between vision and language modalities and accomplishes text recognition via an iterative process.</li>
<li>The proposed LevOCR allows for <font color="#32adff">parallel decoding and dynamic length</font> change, and exhibits <font color="#32adff">good transparency and interpretability</font> in the inference phase, which could be very crucial for diagnosing and improving text recognition models in the future.</li>
</ul>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
      </tags>
  </entry>
  <entry>
    <title>[2022-TIP] PETR: Rethinking the Capability of Transformer-Based Language Model in Scene Text Recognition</title>
    <url>/2022/10/12/PETR-Model/</url>
    <content><![CDATA[<p>This paper utilizes destructed scene text images and predicted length loss. They destruct the images, and design a module to classify whether the input image is destructed or not. Besides, the loss of the predicted label’s length is used to optimize the model. </p>
<a id="more"></a>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><ul>
<li>Visual prediction with low character-wise accuracy limits the capability of transformer-based language model (TLM).</li>
</ul>
<div align="center"> <img src="/2022/10/12/PETR-Model/bg1.jpg" width="400"></div>

<ul>
<li>Inaccurate word length of visual prediction will cause the misalignment problem between vision model and TLM.</li>
</ul>
<div align="center"> <img src="/2022/10/12/PETR-Model/bg2.jpg" width="300"></div>

<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h2><div align="center"> <img src="/2022/10/12/PETR-Model/method.jpg" width="400"></div>

<h2 id="Vision-Model"><a href="#Vision-Model" class="headerlink" title="Vision Model"></a>Vision Model</h2><div align="center"> <img src="/2022/10/12/PETR-Model/vm.jpg" width="400"></div>

<h3 id="Destruction-Learning-Module"><a href="#Destruction-Learning-Module" class="headerlink" title="Destruction Learning Module"></a>Destruction Learning Module</h3><p>DLM classifies the destructed images (cut the original image horizental and disorganize) and restore the disrupted patches.</p>
<p>visual feature -&gt; average pooling, tanh activation layer, fully connected layer —&gt; probability distribution vector $D\in R^{1\times M}$</p>
<p>$D$ is used to classify whether the input is destructed.</p>
<h3 id="visual-Reasoning-Module"><a href="#visual-Reasoning-Module" class="headerlink" title="visual Reasoning Module"></a>visual Reasoning Module</h3><p>VRM uses the linguistic information in the visual context, predict the original and destructed images respectively.</p>
<h2 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h2><p>LM aims to rectify the visual predictions based on learned linguistic rules</p>
<div align="center"> <img src="/2022/10/12/PETR-Model/lm.jpg" width="600"></div>

<h3 id="Language-Rectification-Module"><a href="#Language-Rectification-Module" class="headerlink" title="Language Rectification Module"></a>Language Rectification Module</h3><p>Concatenate the visual features and linguistic features, then use the aggregated feature map through attention to <font color="#32adff">predict the length of the word</font></p>
<h2 id="Learnable-Fusion-Module"><a href="#Learnable-Fusion-Module" class="headerlink" title="Learnable Fusion Module"></a>Learnable Fusion Module</h2><div align="center"> <img src="/2022/10/12/PETR-Model/lfm.jpg" width="600"></div>

<h1 id="Objective-function"><a href="#Objective-function" class="headerlink" title="Objective function"></a>Objective function</h1><p>Loss function: $L = L_V + L_L + \lambda_F L_F, \lambda_F=2$</p>
<h2 id="Vision-Loss-function"><a href="#Vision-Loss-function" class="headerlink" title="Vision Loss function"></a>Vision Loss function</h2><p>The reconstruction loss, destructed classification loss and recognition loss.</p>
<p>Function: $L_V = 0.1L_r + L_d + L_{recV}$</p>
<h2 id="Language-Loss-function"><a href="#Language-Loss-function" class="headerlink" title="Language Loss function"></a>Language Loss function</h2><p>The rectification loss and recognition loss.</p>
<p>Function: $L_L = \gamma L_p + 0.15L_{recL}$</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul>
<li>DLM???</li>
</ul>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>Semantic Module</tag>
      </tags>
  </entry>
  <entry>
    <title>[2022-ECCV] Scene Text Recognition with Single-Point Decoding Network</title>
    <url>/2022/10/12/Scene-Text-Recognition-with-Single-Point-Decoding-Network/</url>
    <content><![CDATA[<a id="more"></a>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1>]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper Review] Scene Text Recognition with Permuted Autoregressive Sequence Models</title>
    <url>/2022/09/13/Parseq-Model/</url>
    <content><![CDATA[<p>This method utilize the idea of XLNet (Permuted language Model), which is an outstanding work in the area of NLP.<br><a id="more"></a></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Due to occlusion, image features alone will not be enough to make accurate inference. Many methods utilize language semantics to aid the recognition.</p>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>Semantic Module</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper Review] MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining</title>
    <url>/2022/07/11/Mask-OCR/</url>
    <content><![CDATA[<p>This paper explores the way to employ <font color="#32adff">MAE’s idea</font> in STR task. It pretrains the STR encoder in a self-supervised manner (formed with N transformer encoder) over unlabeled real text images, while pretraining decoder on synthesized text images using the supervision loss.</p>
<a id="more"></a>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Works like MAE, BEiT proposed methods to pretrain the masked autoencoders, and achieve SOTA performance.</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><div align="center"> <img src="/2022/07/11/Mask-OCR/pipeline.jpg" width="700"></div>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>MAE</tag>
        <tag>Self-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper Review] Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition</title>
    <url>/2022/06/25/Joint-Visual-Semantic-Reasoning/</url>
    <content><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>How to develop a visual-semantic reasoning skill for text recognition?</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>The framework contains a <font color="#32adff">visual feature extractor</font> extracts context-rich holistic feature and multi-scale feature maps, and a <font color="#32adff">multi-stage attentional decoder</font> builds up the character sequence estimates.</p>
<a id="more"></a>
<div align="center"> <img src="/2022/06/25/Joint-Visual-Semantic-Reasoning/pipeline.jpg" width="700"></div>

<h2 id="Visual-Feature-Extractor"><a href="#Visual-Feature-Extractor" class="headerlink" title="Visual Feature Extractor"></a>Visual Feature Extractor</h2><p>Architecture: <strong>ResNet + FPN + 2-layer BiLSTM</strong></p>
<p>The 2-layer BiLSTM takes in a sequence feature $(W_L \times D)$: column-wise max-pooling $B_L$</p>
<ul>
<li>multi-scale feature-maps $\{B_L, B_{L-1}, B_{L-2}\}$: context for 2D attention</li>
<li>holistic feature $h_L$: initialize the initial state of first stage decoder</li>
</ul>
<h2 id="Joint-Visual-Semantic-Reasoning-Decoder"><a href="#Joint-Visual-Semantic-Reasoning-Decoder" class="headerlink" title="Joint Visual-Semantic Reasoning Decoder"></a>Joint Visual-Semantic Reasoning Decoder</h2><p>The first layer decoder relies only on the extracted visual feature. Subsequent stages use global semantic information.</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1>]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>Semantic Module</tag>
      </tags>
  </entry>
  <entry>
    <title>DAN Model</title>
    <url>/2022/06/24/DAN-Model/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>[Paper Review] From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network</title>
    <url>/2022/06/24/From-two-to-one/</url>
    <content><![CDATA[<div align="center"> <img src="/2022/06/24/From-two-to-one/background.jpg" width="400"></div>
<a id="more"></a>

# Background
Previous methods use two-step architecture to utilize the semantic information, However
- the extra computation cost is huge
- the two modal is independent (the semantic module works as a post-processor)

# Method
<div align="center"> <img src="/2022/06/24/From-two-to-one/VisionLAN_pipeline.jpg" width="750"></div>
Backbone (ResNet45) extracts the 2D features, then MLM generates the position-aware character mask map $Mask_c$. VRM takes occluded feature map $V_m$ as input and makes prediction under the complete word-level supervision.

## Masked Language-aware Module
<div align="center"> <img src="/2022/06/24/From-two-to-one/MLM.jpg" width="400"></div>
Aim: generate the character-wise mask map with only original word-level annotations.
- Input: visual features $V$, character index $P\in[1,N_w]$
- Transformer unit and prediction layer for $V_{mas}$ and $V_{rem}$ share the weights.

## Visual Reasoning Module
<div align="center"> <img src="/2022/06/24/From-two-to-one/VRM.jpg" width="400"></div>
N transformer units + prediction layer

# Training
Loss function:
$$L = L_{rec} + \lambda_1L_{mas} + \lambda_2L_{rem}$$

- LF training: split the connection between MLM and VRM, VRM uses visual textual for prediction.
- LA training: VRM learn the linguistic rules.

# Experiments
<div align="center"> <img src="/2022/06/24/From-two-to-one/results.jpg" width="750"></div>

<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h1><ul>
<li>Paper: <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_From_Two_to_One_A_New_Scene_Text_Recognizer_With_ICCV_2021_paper.pdf">From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network</a></li>
<li>Code: <a href="https://github.com/wangyuxin87/VisionLAN">https://github.com/wangyuxin87/VisionLAN</a></li>
</ul>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>Semi-Supervised</tag>
        <tag>Mask</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper Review] Multi-modal Text Recognition Networks: Interactive Enhancements between Visual and Semantic Features</title>
    <url>/2022/06/24/MATRN-Model/</url>
    <content><![CDATA[<div align="center"> <img src="/2022/06/24/MATRN-Model/MATRN_pipeline.jpg" width="750"></div>

<a id="more"></a>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p><strong>Question</strong>: What is the best way to model the interactions between visual and semantic features identified by VM and LM, respectively?</p>
<div align="center"> <img src="/2022/06/24/MATRN-Model/compare.jpg" width="350"></div>

<ul>
<li>(a): <a href="/2022/06/23/Towards-Accurate-Scene-Text-Recognition-with-Semantic-Reasoning-Networks/" title="SRN">SRN</a>,  <a href="/2022/06/23/ABINet-Model/" title="ABINet">ABINet</a></li>
<li>(b): <a href="/2022/06/25/Joint-Visual-Semantic-Reasoning/" title="Joint Visual Semantic Reasoning">Joint Visual Semantic Reasoning</a></li>
<li>(c): <a href="/2022/06/24/From-two-to-one/" title="VisionLAN">VisionLAN</a></li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>For <font color="#32adff">visual and semantic feature extraction</font>, MATRN employ <font color="ffad4">ABINet’s corresponding structures</font>. The LM is initialized with weights pre-trained on WikiText-103.</p>
<p>It proposes new spatial encoding, multi-modal feature enhancement and visual clue masking module.</p>
<h2 id="Spatial-Encoding-to-Semantic-Features"><a href="#Spatial-Encoding-to-Semantic-Features" class="headerlink" title="Spatial Encoding to Semantic Features"></a>Spatial Encoding to Semantic Features</h2><p><strong>Question</strong>: How to align each piece of information of different modalities?<br><strong>Solution</strong>: Utilize $A^{V-S}$ the <font color="ffad4">attention map for text generation</font> and $P^V$ <font color="ffad4">spatial position embedding</font>.</p>
<ul>
<li>$A^{V-S}$ provides which visual features are used to estimated a character at each position.</li>
<li>$S^{Align} = S + A^{V-S}flatten(P^V)$</li>
</ul>
<h2 id="Multi-modal-Features-Enhancement"><a href="#Multi-modal-Features-Enhancement" class="headerlink" title="Multi-modal Features Enhancement"></a>Multi-modal Features Enhancement</h2><div align="center"> <img src="/2022/06/24/MATRN-Model/fe.jpg" width="400"></div>

<font color="32adff">Multi-modal FE</font>: Both visual and semantic features are processed through <font color="ffad4">self-attentions</font>. Contact them together.

## Visual Clue Masking Strategy
<div align="center"> <img src="/2022/06/24/MATRN-Model/mask.jpg" width="400"></div>

Randomly select a single character and hides corresponding visual features based on the attention map $A^{V-S}$:
+ find the <font color="32adff">top-K visual features</font> in the descending order of the attention scores at the chosen position
+ multi-modal FE becomes <font color="ffad4">stimulated to encode semantic knowledge into the visual features</font>

<h2 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective:"></a>Training Objective:</h2><script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{E^v} + \frac{1}{M} \sum\limits_{i=1}\limits^M (\mathcal{L}_{S_{(i)}} + \mathcal{L}_{S^M_{(i)}} + \mathcal{L}_{E^{V^M}_{(i)}} + \mathcal{L}_{F_{(i)}}),</script><p>visual features; semantic, multi-modal semantic, multi-modal visual, final fused features at $i$th iteration</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><div align="center"> <img src="/2022/06/24/MATRN-Model/result.jpg" width="750"></div>

<ul>
<li>Semantic FE: add corresponding visual features into semantic features, <font color="32adff">better regular recognition</font></li>
<li>Visual FE: add corresponding semantic features into visual features, <font color="32adff">better irregular recognition</font></li>
</ul>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Compare with the ABINet:</p>
<ul>
<li>Add multi-modal feature enhancement, let visual features interact with semantic features</li>
<li>Visual clue masking</li>
</ul>
<p><strong>Questions to think about:</strong></p>
<ul>
<li>More concise structure?</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h1><ul>
<li>Paper: <a href="https://arxiv.org/pdf/2111.15263.pdf">Multi-modal Text Recognition Networks: Interactive Enhancements between Visual and Semantic Features</a></li>
<li>Code: <a href="https://github.com/wp03052/MATRN">https://github.com/wp03052/MATRN</a></li>
</ul>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>Semantic Module</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper Review] Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition</title>
    <url>/2022/06/23/ABINet-Model/</url>
    <content><![CDATA[<div align="center"> <img src="/2022/06/23/ABINet-Model/pipeline.jpg" width="700"></div>

<a id="more"></a>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>How to effectively model linguistic rules in end-to-end deep networks remains a research challenge.</p>
<div align="center"> <img src="/2022/06/23/ABINet-Model/compare.jpg" width="380"></div>

<p><strong>Approach:</strong></p>
<ul>
<li><p><font color="#32adff">Autonomous</font>: The learning between vision and language should be independent.</p>
<p>The vision and language modules in previous methods are in serial (Fig. 1a), which makes it cannot utilize the pre-trained LM from large-scale unlabeled text.</p>
</li>
<li><font color="#32adff">Bidirectional</font>: The contextual information should be bidirectional.</li>
<li><font color="#32adff">Iterative</font>: Humans adopt a progressive strategy to improve prediction confidence by iteratively correcting the recognized results.</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>The ABINet model contains vision model and language model. The VM extracts visual features and output the primary prediction tokens to the LM model. And the LM captures the bidirectional semantic information by Transformer units. The visual features and language features are fused using gated unit, and then the fusion prediction can be feed back into the LM repeatedly to feature correction.</p>
<ul>
<li>Autonomous: <font color="#32adff">Block gradient flow between VM and LM</font>, both VM and LM could be pretrained separately.</li>
<li>Bidirectional: <font color="#32adff">several Transformer layers</font></li>
<li>Iterative: <font color="#32adff">Feed the outputs of ABINet into LM repeatedly</font>.</li>
</ul>
<h2 id="Vision-Model"><a href="#Vision-Model" class="headerlink" title="Vision Model"></a>Vision Model</h2><ul>
<li>Backbone: <font color="#ffad4">ResNet + Transformer units</font></li>
<li>Position Attention:<ul>
<li><font color="#ffad4">Query: positional encodings of character orders</font></li>
<li>Key: a mini U-Net’s output with input features</li>
<li>V: input features</li>
<li>Attention Score: dot-product</li>
</ul>
</li>
</ul>
<div align="center"> <img src="/2022/06/23/ABINet-Model/vision.jpg" width="400"></div>

<h2 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h2><p>A variant of L-layers transformer decoder (multi-head attention and feed-forward network followed by residual connection and layer normalization).</p>
<p>Attention operation <font color="#ffad4">(No Self-Attention)</font>:</p>
<ul>
<li><font color="#32adff">Q: positional encodings of character orders in the first layer. </font></li>
<li>K, V: character probability</li>
<li>Attention Score: dot-product</li>
</ul>
<div align="center"> <img src="/2022/06/23/ABINet-Model/language.jpg" width="320"></div>

<h2 id="Fusion-and-Supervised-Training"><a href="#Fusion-and-Supervised-Training" class="headerlink" title="Fusion and Supervised Training"></a>Fusion and Supervised Training</h2><p>Gated mechanism:</p>
<ul>
<li><script type="math/tex; mode=display">G = \sigma([F_v, F_l]W_f)</script></li>
<li><script type="math/tex; mode=display">F_f = G\odot F_v + (1-G)\odot F_l</script></li>
</ul>
<p>Total loss:</p>
<script type="math/tex; mode=display">\mathcal{L} = \lambda_v\mathcal{L}_v + \frac{\lambda_l}{M} \sum\limits_{i=1}\limits^M\mathcal{L}_l^i + \frac{1}{M} \sum\limits_{i=1}\limits^M\mathcal{L}_f^i,</script><p>cross entropy losses from $F_v$, $F_l$ and $F_f$ at $i$th iteration</p>
<h2 id="Semi-supervised-Ensemble-Self-training"><a href="#Semi-supervised-Ensemble-Self-training" class="headerlink" title="Semi-supervised Ensemble Self-training"></a>Semi-supervised Ensemble Self-training</h2><div align="center"> <img src="/2022/06/23/ABINet-Model/algorithm.jpg" width="500"></div>

<ul>
<li>Equation 8: the loss function for supervised training</li>
<li>Equation 9:<ul>
<li><script type="math/tex; mode=display">\mathcal{C} = \mathop{min}\limits_{1\le t\le T}e^{\mathbb{E}[logP(y_t)]}</script></li>
<li><script type="math/tex; mode=display">P(y_t) = \mathop{max}\limits_{1\le m\le M}P_m(y_t)</script></li>
</ul>
</li>
<li>$N_{upl}$: the step number for updating pseudo labels</li>
</ul>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><div align="center"> <img src="/2022/06/23/ABINet-Model/results.jpg" width="750"></div>

<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Compare with the SRN:</p>
<ul>
<li>Block the gradient between the LM and VM</li>
<li>Modify the parallel attention</li>
<li>LM layers does not use self-attention</li>
</ul>
<p><strong>Questions to think about:</strong></p>
<ul>
<li><font color="#32adff">"No self-attention is applied in BCN to avoid leaking information across time steps" ??</font></li>
<li><font color="#32adff">Although this paper analyzes the better way to combine the visual and semantic information compared with SRN, it seems that there is still room for improvement.</font>

</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h1><ul>
<li>Paper: <a href="https://https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Read_Like_Humans_Autonomous_Bidirectional_and_Iterative_Language_Modeling_for_CVPR_2021_paper.pdf">Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition</a></li>
<li>Code: <a href="https://github.com/FangShancheng/ABINet">https://github.com/FangShancheng/ABINet</a></li>
</ul>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>Semantic Module</tag>
        <tag>Parallel Decoder</tag>
        <tag>Iterative Correction</tag>
        <tag>Semi-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper Review] Towards Accurate Scene Text Recognition with Semantic Reasoning Networks</title>
    <url>/2022/06/23/Towards-Accurate-Scene-Text-Recognition-with-Semantic-Reasoning-Networks/</url>
    <content><![CDATA[<div align="center"> <img src="/2022/06/23/Towards-Accurate-Scene-Text-Recognition-with-Semantic-Reasoning-Networks/SRN_pipeline.jpg" width="750"></div>

<a id="more"></a>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><ul>
<li>Better utilize the semantic information to boost the recognition accuracy for the hard images (blurred, shaded, low resolution).</li>
<li>Recent works consider the semantic information in the way of one-way serial transmission.</li>
</ul>
<h1 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h1><p>As shown in the above figure, the SRN consists of Backbone, Parallel Visual Attention Module (PVAM), Global Semantic Reasoning Module (GSRM) and Fusion Module. For <font color="#32adff">backbone</font>, it uses the <font color="#ffad4">ResNet50 + FPN</font> to extract the local features and two stack <font color="#ffad4">transformer units</font> to capture the global spatial dependencies.</p>
<h2 id="PVAM"><a href="#PVAM" class="headerlink" title="PVAM"></a>PVAM</h2><p>The parallel attention decoder cannot get the semantic information between characters.</p>
<ul>
<li><font color="#32adff">Query: the reading order</font> $[0,1,…,N-1]$</li>
<li>Key and Value: the input 2D features $(v_{ij}, v_{ij})$</li>
<li>Attention Score: multi-layer perceptron (For more information, see <a href="/2022/01/23/Attention-Transformer/" title="the post about attention">the post about attention</a>.)</li>
</ul>
<h2 id="GSRM"><a href="#GSRM" class="headerlink" title="GSRM"></a>GSRM</h2><div align="center"> <img src="/2022/06/23/Towards-Accurate-Scene-Text-Recognition-with-Semantic-Reasoning-Networks/GSRM.jpg" width="380"></div>

<ul>
<li>Visual-to-Semantic Embedding Block: Predict the text token utilizing the PVAM’s output.</li>
<li>Global Semantic Reasoning Block: <font color="#32adff">several transformer units</font> (multi-head self-attention, masked the corresponding character itself)</li>
</ul>
<h2 id="Fusion"><a href="#Fusion" class="headerlink" title="Fusion"></a>Fusion</h2><p>SRN utilizes the <font color="#32adff">gated unit</font> to fuse the visual aligned features G and semantic information S.</p>
<p>$Loss = \alpha_eL_e + \alpha_rL_r + \alpha_fL_f$ (1.0, 0.15, 2.0)</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h2><ul>
<li>Utilize the ResNet50 pre-trained on ImageNet as the initialized model.</li>
<li>Train SRN without GSRM for about 3 epochs.</li>
<li>Then, train the whole pipeline end-to-end with the same optimizer until convergence.</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><div align="center"> <img src="/2022/06/23/Towards-Accurate-Scene-Text-Recognition-with-Semantic-Reasoning-Networks/results.jpg" width="800"></div>

<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>SRN decouples the semantic module from the decoding process and uses parallel attention (with idx query) to decode the visual features instead. It utilizes the tokens predicts by visual module to get the semantic information, and then fuse the two modal features.</p>
<p>The serial architectural gets severely limited on using <font color="ffad4">argmax operation</font> in visual-to-semantic embedding layer, which invokes <font color="ffad4">non-differentiability</font>.</p>
<p><strong>Questions to think about</strong>:</p>
<ul>
<li><font color="#32adff">What's the best way to interact semantic information with visual information?</font></li>
<li><font color="#32adff">Can the parallel attention be improved (Parallel Attention v.s. CTC)?</font></li>
<li><font color="#32adff">What if we train the semantic module with the ground truth?</font>

</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h1><ul>
<li>Paper: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Towards_Accurate_Scene_Text_Recognition_With_Semantic_Reasoning_Networks_CVPR_2020_paper.pdf">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks</a></li>
<li>Code: <a href="https://github.com/chenjun2hao/SRN.pytorch">https://github.com/chenjun2hao/SRN.pytorch</a></li>
</ul>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>Semantic Module</tag>
        <tag>Parallel Decoder</tag>
      </tags>
  </entry>
  <entry>
    <title>Review the Scene Text Recognition Models with Semantic Information</title>
    <url>/2022/06/23/Review-the-Scene-Text-Recognition-Models-with-Semantic-Information/</url>
    <content><![CDATA[<p>This article reviews the ways that scene text recognition models utilize the semantic information to boost their performance, from the <font color="#32adff">Bahdanau Attention Decoder</font> to various <font color="#32adff">Semantic Modules</font>.<br><a id="more"></a></p>
<p>As we all know, to recognize the text in the wild, both the visual features and semantic information are significant, Especially for the blurred images. The ideal text recognition process is recognizing the clear text using visual features and inferring the blurred words according to its context information.</p>
<h2 id="Bahdanau-Attention"><a href="#Bahdanau-Attention" class="headerlink" title="Bahdanau Attention"></a>Bahdanau Attention</h2><p>RARE, ASTER and so on utilizes the <font color="#32adff"> recurrence attention</font> to capture the semantic information during the decoding process. At time step $t$, the decoder predicts a character based on previous state $s_{t-1}$ and the sequence features $h_{t}$ output by BLSTM.</p>
<div align="center"> <img src="/2022/06/23/Review-the-Scene-Text-Recognition-Models-with-Semantic-Information/RARE_architecture.jpg" width="300"></div>

<p><strong>Drawbacks:</strong></p>
<ul>
<li><font color="#ffad4">Limited semantic context</font> (one-way serial)</li>
<li>Pass the wrong semantic information down and cause a <font color="#ffad4">error accumulation</font></li>
<li>Time-consuming and <font color="#ffad4">inefficient</font></li>
</ul>
<h2 id="SEED-2020-CVPR"><a href="#SEED-2020-CVPR" class="headerlink" title="SEED (2020 CVPR)"></a>SEED (2020 CVPR)</h2><p><strong>Drawbacks:</strong><br>For example, two related words “Chair” and “Table” may lie close in word-embedding space, but their character combination is apart.</p>
<h2 id="SRN-2020-CVPR"><a href="#SRN-2020-CVPR" class="headerlink" title="SRN (2020 CVPR)"></a>SRN (2020 CVPR)</h2><a href="/2022/06/23/Towards-Accurate-Scene-Text-Recognition-with-Semantic-Reasoning-Networks/" title="SRN">SRN</a> proposes a structure named global semantic reasoning module (GSRM), which is <font color="#32adff">multi-way parallel transmission</font>. The semantic module works like a post-processor.
- PVAM: Parallel attention mechanism with idx queries.
- GSRM: Four Transformer layers (multi-head self-attention, masked the corresponding character itself)

<div align="center"> <img src="/2022/06/23/Review-the-Scene-Text-Recognition-Models-with-Semantic-Information/SRN_pipeline.jpg" width="750"></div>

**Drawbacks:**
The serial connection between the visual and semantic module may not be the perfect interaction:
- Cannot use the pre-trained LM from large-scale unlabeled text directly.
- Errors from VM output directly harm the LM accuracy.

## ABINet (2021 CVPR)
<a href="/2022/06/23/ABINet-Model/" title="ABINet">ABINet</a> proposes an <font color="#32adff">autonomous, bidirectional and Iterative</font> structure.

- Autonomous: Block gradient flow between VM and LM, both VM and LM could be pretrained separately.
- Bidirectional: several Transformer decoder layers (no self-attention, query is the positional encodings of the characters in the first layer)
- Iterative: Feed the outputs of ABINet into LM repeatedly.

<div align="center"> <img src="/2022/06/23/Review-the-Scene-Text-Recognition-Models-with-Semantic-Information/ABINet_pipeline.jpg" width="700"></div>

**Drawbacks:**
The LM character refinements does not <font color="#ffad4">interact with visual features</font>, like a post-processor. (The LM's inputs are logits[N, T, C] and positional encodings.)

## VisionLAN (2021 ICCV)
<a href="/2022/06/24/From-two-to-one/" title="VisionLAN">VisionLAN</a>
<div align="center"> <img src="/2022/06/23/Review-the-Scene-Text-Recognition-Models-with-Semantic-Information/VisionLAN_pipeline.jpg" width="700"></div>

<h2 id="MATRN"><a href="#MATRN" class="headerlink" title="MATRN"></a>MATRN</h2><p><a href="/2022/06/24/MATRN-Model/" title="MATRN">MATRN</a> summarizes the previous methods, and proposes an improvement model based on ABINet, which <font color="#32adff">emphasizes the interaction between visual and semantic features</font>.</p>
<div align="center"> <img src="/2022/06/23/Review-the-Scene-Text-Recognition-Models-with-Semantic-Information/MATRN_pipeline.jpg" width="700"></div>

<p><strong>Drawbacks:</strong></p>
<font color="ffad4"> Somewhat complex </font>

<h2 id="PARSeq"><a href="#PARSeq" class="headerlink" title="PARSeq"></a>PARSeq</h2><p><a href="/2022/09/13/Parseq-Model/" title="“PAarseq”} utilizes the &lt;&#x2F;p&gt;">“PAarseq”} utilizes the &lt;&#x2F;p&gt;</a></p>]]></content>
      <categories>
        <category>Scene Text Recognition</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>Review</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper Review] SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition</title>
    <url>/2022/03/25/SEED-Model/</url>
    <content><![CDATA[<p><div align="center"> <img src="/2022/03/25/SEED-Model/pipeline.jpg" width="750"></div><br><a id="more"></a></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h1><ul>
<li>Paper: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_SEED_Semantics_Enhanced_Encoder-Decoder_Framework_for_Scene_Text_Recognition_CVPR_2020_paper.pdf">SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition</a></li>
<li>Code: <a href="https://github.com/Pay20Y/SEED">https://github.com/Pay20Y/SEED</a></li>
</ul>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>Semantic Module</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention &amp; Transformer</title>
    <url>/2022/01/23/Attention-Transformer/</url>
    <content><![CDATA[<h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>Attention mechanism mimics the retrieval of a value $v_i$ for a query $q$ based on a key $k_i$ in database.</p>
<a id="more"></a>
<div align="center"> <img src="/2022/01/23/Attention-Transformer/1.png" width="650"></div>

<div align="center"> <img src="/2022/01/23/Attention-Transformer/2.png" width="500"></div>

<p><strong>Ways to compute the attention scores:</strong></p>
<div align="center"> <img src="/2022/01/23/Attention-Transformer/3.png" width="650"></div>

<h2 id="Bahdanau-Attention"><a href="#Bahdanau-Attention" class="headerlink" title="Bahdanau Attention"></a>Bahdanau Attention</h2><p>Attention is used between decoder steps: state $h_{t-1}$ is used to compute attention and its output $c(t)$, and both $h_{t-1}$ and $c(t)$ are passed to the decoder at step $t$.</p>
<ul>
<li>ASTER use this attention</li>
<li>SEED use the semantic embedding to initialize the c(0)</li>
</ul>
<div align="center"> <img src="/2022/01/23/Attention-Transformer/4.png" width="650"></div>

<h2 id="Luong-Attention"><a href="#Luong-Attention" class="headerlink" title="Luong Attention"></a>Luong Attention</h2><p>Attention is used after RNN decoder step $t$ before making a prediction. State $h_{t}$ used to compute attention and its output $c(t)$. The $h_t$ is combined with $c(t)$ to get an updated representation $\tilde{h}_t$, which is used to get a prediction.</p>
<div align="center"> <img src="/2022/01/23/Attention-Transformer/5.png" width="650"></div>

<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Get the relations between x and each word, and then add the weighted summation into x.</p>
<div align="center"> <img src="/2022/01/23/Attention-Transformer/6.png" width="650"></div>

<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><div align="center"> <img src="/2022/01/23/Attention-Transformer/7.png" width="550"></div>

<div align="center"> <img src="/2022/01/23/Attention-Transformer/8.png" width="650"></div>

<div align="center"> <img src="/2022/01/23/Attention-Transformer/9.png" width="650"></div>

<div align="center"> <img src="/2022/01/23/Attention-Transformer/10.png" width="650"></div>

<p>Resource:</p>
<ul>
<li>Thanks to Voita’s post, <a href="https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html">Seq2Seq and Attention</a></li>
<li><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All you Need</a></li>
</ul>
]]></content>
      <categories>
        <category>Basic Neural Networks</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design and Analysis</title>
    <url>/2021/09/10/Algorithm-Design-and-Analysis/</url>
    <content><![CDATA[<p>Practical Problems:</p>
<ul>
<li>step1: topic choosing<br>A practical problem</li>
<li>step 2: formulate the problem<a id="more"></a>
</li>
</ul>
<p>Key observation:</p>
<ul>
<li>problem structure (decompose, simple case)</li>
<li>solution (space, solution —&gt; solution)</li>
</ul>
<p>Math Problems</p>
<ul>
<li><p><strong>POSITIVE</strong>, design an algorithm to solve this problem</p>
<ul>
<li><p>P0: SIMPLE CASE (try solving the simplest case of the certain problem) —&gt; P1: DECOMPOSE —&gt; INDUCTION —&gt;DC OPTIMAL SUBSTRUCTION —&gt; DP P1.2. “near-sighted” —&gt; Greedy</p>
</li>
<li><p>Cannot decompose / —&gt; P2 (SOLUTION —&gt; SOLUTION ) —&gt; S2 IMPROVEMENT (LP/QP/NLP/NF/LS/SA)</p>
</li>
<li><p>p3 SOLUTION FORM<br>$X = [x_1,x_2,…,x_n], x_i = 0/1$; step-by-step construction</p>
</li>
</ul>
<p>ENUMERATION</p>
<p>S3 SMART ENUMERATION (b&amp;b,b&amp;c,b&p; bt)</p>
</li>
</ul>
<ul>
<li><strong>NEGATIVE</strong>, prove the difficulty of it —&gt; TRADE-OFF<ul>
<li>OPTIMAL =&gt; approximation</li>
<li>Deterministic =&gt; Random</li>
<li>Worst-Case =&gt; Practical Cases</li>
</ul>
</li>
</ul>
<p>A: active set (the set needing to be extract)</p>
<ul>
<li>Estimate the length of the line after extracting the whole set</li>
<li>Estimate the lower bound of the possible extractions.</li>
</ul>
<p>TAKE-HOME MSG</p>
<ul>
<li>Try to solve the simplest case first when meeting a practical problem.<br>-</li>
<li>According to the observation of little amount of samples, get heuristic —&gt; NN (large amount of samples)<br>-</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Connectionist Temporal Classification with Maximum Entropy Regularization</title>
    <url>/2021/08/05/Connectionist-Temporal-Classification-with-Maximum-Entropy-Regularization/</url>
    <content><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>CTC tends to produce <font color="32adff">highly peaky and overconfident distributions</font>, which is a symptom of <strong>overfitting</strong>.</p>
<ul>
<li>Harm the training process: CTC lacks exploration and is prone to fall into worse local minima.</li>
<li>Output overconfident paths: CTC tends to concentrate all its output distribution over one specific path.</li>
<li>Output paths with peaky distribution.</li>
</ul>
<a id="more"></a>
<p><strong>CTC peaky distribution problem</strong>:</p>
<ul>
<li>Once CTC finds a dominant feasible path during the training process, the <font color="32adff">error signal will concentrate on the vicinity of this path</font>, and the prediction of this feasible path will continuously strengthen until this path completely dominates the prediction output.</li>
<li>As blanks are included in most of the feasible paths, <font color="32adff">dominant paths are often overwhelmed by blanks</font>, interspersed by sharp spikes (narrow regions along the time axis) of non-blank labels.</li>
</ul>
<h1 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul>
<li><p>Propose a maximum conditional entropy based regularization for CTC (EnCTC)</p>
<p>The formula of maximum conditional entropy is: <font color="ffaad4">$H(p(\pi|l,X))=-\sum\limits_{\pi\in \mathcal{B}^{-1}(l)}p(\pi|X,l)log\,p(\pi|X,l)$</font>.</p>
<p>The graph of function $f=-xlog(x)$ in the range of $x\in(0,1]$ is shown below.</p>
<div align="center"> <img src="/2021/08/05/Connectionist-Temporal-Classification-with-Maximum-Entropy-Regularization/entropy.jpg" width="400"></div>

<p>From the graph, we can see that when $x$ near $0$ or $1$, $f$ is near to $0$.</p>
</li>
<li><p>Propose an algorithm to limit the size of the CTC feasible set by eliminating these unreasonable paths that seriously violate the equal spacing prior (EsCTC)</p>
</li>
</ul>
<h2 id="CTC"><a href="#CTC" class="headerlink" title="CTC"></a>CTC</h2><p>The original CTC loss function is</p>
<font color="ffaad4">$$L_{ctc} = -log\,p(l|X_{1:T}) = -log\; \sum\limits_{\pi\in\mathcal{B}^{-1}(l)}p(\pi|X_{1:T}),$$</font>
where the probability of $\pi$ is defined as
<font color="ffaad4">$$p(\pi|X_{1:T}) = \mathop{\Pi}\limits_{t=1}\limits^Ty_{\pi_t}^t, \forall\pi\in L'^T.$$</font>

<p>The error signal of CTC loss with respect to $y_k^t$ is:</p>
<div align="center"> <img src="/2021/08/05/Connectionist-Temporal-Classification-with-Maximum-Entropy-Regularization/partial.jpg" width="400"></div>

<h2 id="EnCTC"><a href="#EnCTC" class="headerlink" title="EnCTC"></a>EnCTC</h2><h2 id="EsCTC"><a href="#EsCTC" class="headerlink" title="EsCTC"></a>EsCTC</h2><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><ul>
<li><a href="https://papers.nips.cc/paper/2018/hash/e44fea3bec53bcea3b7513ccef5857ac-Abstract.html">Connectionist Temporal Classification with Maximum Entropy Regularization</a></li>
<li><a href="https://github.com/liuhu-bigeye/enctc.crnn">Github Code</a></li>
</ul>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>CTC</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper-Review] Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning</title>
    <url>/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/</url>
    <content><![CDATA[<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/authors.jpg" width="700"></div>

<a id="more"></a>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Alleviate the issue of over-fitting and improve generalization performance on testing data:</p>
<ul>
<li>Regularization-based methods – introduce penalty on the complexity of the model.</li>
<li>Data augmentation techniques – leverage important invariance properties of the data.</li>
</ul>
<font color="32adff">The main goal of Generalization is to make the AI system to perform better on the test data.</font>


<h1 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Virtual adversarial direction:</p>
<ul>
<li>a direction of the perturbation that can most greatly alter the output distribution in the sense of distributional divergence.</li>
<li>can be defined on unlabeled data point.</li>
</ul>
<font color="32adff">Add a regularization item (LDS) to the original loss, LDS is the divergence based distributional robustness of the model against virtual adversarial direction. </font>

<script type="math/tex; mode=display">\mathcal{l}(\mathcal{D}_l, \theta) + \alpha\mathcal{R}(\mathcal{D}_l,\mathcal{D}_{ul},\theta)</script><h2 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h2><ul>
<li>$x\in R^I, y\in Q$: input vector, output label</li>
<li>$p(y|x,\theta)$: output distribution</li>
<li>$\hat{\theta}$: vector of the model parameters at a specific iteration step of the training process</li>
<li>$\mathcal{D}_l=\{x_l^{(n)},y_l^{(n)}|n=1,\dots,N_l\}$: a labeled dataset</li>
<li>$\mathcal{D}_{ul}=\{x_{ul}^{(m)}|m=1,\dots,N_l\}$: an unlabeled dataset</li>
<li>$D[p,p’]$: non-negative function measures the divergence between two distributions $p$ and $p’$, for example the cross entropy: $D[p,p’] = -\sum_ip_ilogp’_i$</li>
</ul>
<p>Thus, the formulation  of the adversarial training is:</p>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/func1.jpg" width="400"></div>

<h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><p>Let $x_*$ represent either $x_l$ or $x_{ul}$ (full label information is not available at all times,</p>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/func2.jpg" width="400"></div>

<p>Use “virtual” labels that are probabilistically generated from $p(y|x,\theta)$ in place of labels that are unknown to the user, and compute adversarial direction based on the virtual labels.</p>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/func3.jpg" width="400"></div>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/func4.jpg" width="400"></div>

<p>The regularization term is the average of $LDS(x_*,\theta)$ over all input data points:</p>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/func5.jpg" width="450"></div>

<p>The full objective function is $\mathcal{l}(\mathcal{D}_l, \theta) + \alpha\mathcal{R}(\mathcal{D}_l,\mathcal{D}_{ul},\theta)$, where $\mathcal{l}(\mathcal{D}_l,\theta)$ is the negative log-likelihood for the labeled dataset.</p>
<font color="32adff"> Virtual adversarial training (VAT) is a training method with the regularizer $R_{vadv}$. </font>

<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>VAT is an algorithm that updates the model by the weighted sum of the gradient of the likelihood and the gradient $\nabla_{\theta R_{adv}}$ computed with Algorithm 1.</p>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/alg.jpg" width="450"></div>

<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Generalization-ability"><a href="#Generalization-ability" class="headerlink" title="Generalization ability"></a>Generalization ability</h2><p>Efficacy of VAT on Benchmark Tasks: supervised</p>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/exp1.jpg" width="450"></div>

<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/exp2.jpg" width="450"></div>

<p>Efficacy of VAT on Benchmark Tasks: semi-supervised</p>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/exp3.jpg" width="450"></div>

<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/exp4.jpg" width="450"></div>

<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>Effects of perturbation size $\epsilon$ and regularization coefficient $\alpha$:</p>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/exp5.jpg" width="450"></div>

<p>For small $\epsilon$, the hyperparameter $\alpha$ plays a similar role as $\epsilon$:</p>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/the1.jpg" width="400"></div>
<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/the2.jpg" width="400"></div>

<h2 id="Adversarial-Robustness"><a href="#Adversarial-Robustness" class="headerlink" title="Adversarial Robustness"></a>Adversarial Robustness</h2><div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/adv1.jpg" width="500"></div>

<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/adv2.jpg" width="500"></div>

<div align="center"> <img src="/2021/08/03/Virtual-Adversarial-Training-A-Regularization-Method-for-Supervised-and-Semi-Supervised-Learning/adv3.jpg" width="500"></div>

<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Virtual adversarial training:<br>Updates the model by <font color="32adff">the weighted sum</font> of the gradient of <font color="32adff">the likelihood and the LDS.</font></p>
<ul>
<li>applicability to semi-supervised learning tasks</li>
<li>applicability to any parametric models for which we can evaluate the gradient with respect to input and parameter</li>
<li>small number of hyperparameters $(\alpha, \epsilon)$</li>
</ul>
<h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/8417973">Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning</a></p>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Adversarial Training</tag>
        <tag>Improve Generalization</tag>
        <tag>Semi-Supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Mathematical Symbols in Latex</title>
    <url>/2021/07/20/Mathematical-Symbols-in-Latex/</url>
    <content><![CDATA[<p><strong>This blog lists most of the mathematical symbols in latex for usage.</strong></p>
<a id="more"></a>
<ul>
<li>$\mathbb{S}$: “\mathbb{S}”</li>
<li>$\mathcal{L}$: “\mathcal{L}”</li>
<li>$\mathscr{L}$: “\mathscr{L}”</li>
<li>$\sum\limits_{i=1}\limits^n$: “\sum\limits_{i=1}\limits^n”</li>
<li>$\mathop{min}\limits_{i\in R}$: \mathop{min}\limits_{i\in R}</li>
<li>$\underline{p_A}$: “\underline{p_A}”</li>
</ul>
<h1 id="Operators"><a href="#Operators" class="headerlink" title="Operators"></a>Operators</h1><div align="center"> <img src="/2021/07/20/Mathematical-Symbols-in-Latex/operators.jpg" width="650"></div>

<h1 id="Relations"><a href="#Relations" class="headerlink" title="Relations"></a>Relations</h1><div align="center"> <img src="/2021/07/20/Mathematical-Symbols-in-Latex/relations.jpg" width="650"></div>

<p>Negations of many of these relations can be formed by just putting \not before the symbol, or by slipping an “n” between the \ and the word. Here are a couple examples, plus many other negations; it works for many of the many others as well.</p>
<div align="center"> <img src="/2021/07/20/Mathematical-Symbols-in-Latex/relations_not.jpg" width="550"></div>

<h1 id="Greek-Letters"><a href="#Greek-Letters" class="headerlink" title="Greek Letters"></a>Greek Letters</h1><div align="center"> <img src="/2021/07/20/Mathematical-Symbols-in-Latex/greek_letters.jpg" width="700"></div>

<h1 id="Arrows"><a href="#Arrows" class="headerlink" title="Arrows"></a>Arrows</h1><div align="center"> <img src="/2021/07/20/Mathematical-Symbols-in-Latex/arrows.jpg" width="550"></div>

<h1 id="Dots"><a href="#Dots" class="headerlink" title="Dots"></a>Dots</h1><div align="center"> <img src="/2021/07/20/Mathematical-Symbols-in-Latex/dots.jpg" width="350"></div>

<h1 id="Accents"><a href="#Accents" class="headerlink" title="Accents"></a>Accents</h1><div align="center"> <img src="/2021/07/20/Mathematical-Symbols-in-Latex/accents.jpg" width="700"></div>

<h1 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h1><div align="center"> <img src="/2021/07/20/Mathematical-Symbols-in-Latex/others.jpg" width="600"></div>

<p>Resource:</p>
<ul>
<li><a href="https://artofproblemsolving.com/wiki/index.php/LaTeX:Symbols">LaTeX:Symbols</a></li>
<li><a href="https://artofproblemsolving.com/wiki/index.php/LaTeX:Commands">LaTeX:Commands</a></li>
</ul>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Latex</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper-Review] Regularizing Neural Networks via Adversarial Model Perturbation</title>
    <url>/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/</url>
    <content><![CDATA[<div align="center"> <img src="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/authors.jpg" width="650"></div>

<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Alleviate the issue of over-fitting and improve generalization performance on testing data:<br><a id="more"></a></p>
<ul>
<li>Regularization-based methods – introduce penalty on the complexity of the model.</li>
<li>Data augmentation techniques – leverage important invariance properties of the data.</li>
</ul>
<p>The model parameter that corresponds to a flat minimum of the empirical risk tends to generalize better.</p>
<h1 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>This work sets out to develop a powerful regularization scheme under the principle of finding flat local minima of the empirical risk.</p>
<div align="center"> <img src="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/fig1.jpg" width="450"></div>

<p>Adversarial Model Perturbation(AMP) minimizes an alternative “AMP loss”:</p>
<font color="32adff">The AMP loss $L_{AMP}(\theta)$ at a parameter setting $\theta$, is the worst (or highest) empirical risk of all perturbations of $\theta$ with the perturbation norm no greater than a small value $\epsilon$.</font>

<font color="#ffaad4">$$L_{AMP}(\theta):=\mathop{max}\limits_{\Delta:\Vert\Delta\Vert\le\epsilon}L_{ERM}(\theta+\Delta)$$</font>

<p>The process of obtaining the AMP loss from the empirical risk can be seen as <font color="32adff">a “max-pooling” operation</font>: slide a window of width $2\epsilon$ across the parameter space and, at each location, returns the maximum value inside the window.</p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><ul>
<li><font color="#ffaad4">$L_{ERM}(\theta):=\frac{1}{\lvert D\rvert}\sum\limits_{(x,y)\in D}l(x,y;\theta)$</font></li>
<li><font color="#ffaad4">$L_{AMP}(\theta):=\mathop{max}\limits_{\Delta\in B(0;\epsilon)}\frac{1}{\lvert D\rvert}\sum\limits_{(x,y)\in D}l(x,y;\theta)$, $B(\mu, \epsilon):=\{\theta\in\Theta:=\lVert\theta-\mu\rVert\le\epsilon\}$</font>, $\epsilon$ is a small positive value serving as a hyperparameter.</li>
</ul>
<p>Consider a mini-batched training approach:</p>
<div align="center"> <img src="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/fig2.jpg" width="300"></div>

<p>Then, we can get:</p>
<div align="center"> <img src="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/fig3.jpg" width="300"></div>

<p>For the algorithm below:</p>
<ul>
<li><p>Inner maximization: N steps are used to <font color="32adff">update $\Delta_B$ in the direction of increasing the ERM loss (PGD attack idea)</font> so as to obtain $J_{AMP,B}$.</p>
</li>
<li><p>Outer minimization: Loops over random batches and minimizes $E_BJ_{AMP,B}$ using mini-batched SGD.</p>
</li>
</ul>
<div align="center"> <img src="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/fig1.jpg" width="450"></div>

<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Generalization-ability"><a href="#Generalization-ability" class="headerlink" title="Generalization ability"></a>Generalization ability</h2><p>RMP: apply a random perturbation to the parameter</p>
<div align="center"> <img src="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/exp_1.jpg" width="600"></div>

<p>Improvement over data augmentation:</p>
<div align="center"> <img src="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/exp2.jpg" width="450"></div>

<p>Loss values v.s. $\theta$:</p>
<div align="center"> <img src="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/exp3.jpg" width="450"></div>

<h2 id="Adversarial-Robustness"><a href="#Adversarial-Robustness" class="headerlink" title="Adversarial Robustness"></a>Adversarial Robustness</h2><p>The flat minima make the adversarial attacks take more efforts for the input to leave the minima.</p>
<p>AMP is expected to improve the model’s adversarial robustness.</p>
<div align="center"> <img src="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/exp4.jpg" width="450"></div>

<h2 id="Computational-Cost"><a href="#Computational-Cost" class="headerlink" title="Computational Cost"></a>Computational Cost</h2><p>Computational cost significantly increase as inner iteration number N grows.</p>
<p>N=1 is sufficient to regularize the neural networks <font color="32adff">(FGSM attack idea)</font>.</p>
<p>For N=1, AMP takes around 1.8x that of ERM training.</p>
<p><strong>AFFORDABLE</strong></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>AMP Idea: minimize over <font color="32adff">AMP loss, the worst (or highest) empirical risk of all perturbations of $\theta$ with the perturbation norm no greater than a small value $\epsilon$.</font></p>
<ul>
<li>Improve the generation performance (image classification, 3D point cloud classification).</li>
<li>Increase the adversarial robustness to some degree</li>
</ul>
<p>Resource: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Regularizing_Neural_Networks_via_Adversarial_Model_Perturbation_CVPR_2021_paper.pdf">Regularizing Neural Networks via Adversarial Model Perturbation</a></p>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Adversarial Training</tag>
        <tag>Improve Generalization</tag>
      </tags>
  </entry>
  <entry>
    <title>Add Mathematical Formula in Hexo-Next Blogs</title>
    <url>/2021/07/20/Add-Mathematical-Formula-in-Hexo-Next-Blogs/</url>
    <content><![CDATA[<p>This blog is about how to add mathematical formulas for your hexo-next blogs.</p>
<p><strong><font color="#32adff">Change the rendering engine of Hexo’s Markdown:</font></strong></p>
<a id="more"></a>
<p>The hexo-renderer-kramed engine is modified based on the default rendering engine hexo-renderer-marked by rewriting the bugs. They are close to each other, and both of them are light weight.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<p><strong><font color="#32adff">Resolve semantic conflicts:</font></strong></p>
<p>Find the file “node_modules\kramed\lib\rules\inline.js”, and make the following two changes.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">//escape: /^\\([\\`*&#123;&#125;\[\]()<span class="comment">#$+\-.!_&gt;])/,</span></span><br><span class="line">escape: /^\\([`*\[\]()<span class="comment">#$+\-.!_&gt;])/,</span></span><br></pre></td></tr></table></figure>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure>
<p><strong><font color="#32adff">Enable mathjax in the _config.yml of Next theme:</font></strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">math:</span><br><span class="line">  <span class="comment"># Default (false) will load mathjax / katex script on demand.</span></span><br><span class="line">  <span class="comment"># That is it only render those page which has `mathjax: true` in Front-matter.</span></span><br><span class="line">  <span class="comment"># If you set it to true, it will load mathjax / katex srcipt EVERY PAGE.</span></span><br><span class="line">  every_page: false</span><br><span class="line"></span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true <span class="comment"># change this to true</span></span><br><span class="line"></span><br><span class="line">  katex:</span><br><span class="line">    enable: false</span><br><span class="line">    <span class="comment"># See: https://github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex</span></span><br><span class="line">    copy_tex: false</span><br></pre></td></tr></table></figure>
<p><strong><font color="#32adff">Enable mathjax for a post:</font></strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">title: Add Mathematical Formula <span class="keyword">in</span> Hexo-Next Blogs</span><br><span class="line">date: <span class="number">2021</span>-07-<span class="number">20</span> <span class="number">14</span>:<span class="number">43</span>:<span class="number">22</span></span><br><span class="line">tags:</span><br><span class="line">  - Hexo</span><br><span class="line">  - Latex</span><br><span class="line">categories: <span class="string">&quot;Hexo&quot;</span></span><br><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><br><strong><font color="#32adff">Use a single $ or $$ to add mathematical formulas.</font></strong></p>
<p>Resource: <a href="https://blog.csdn.net/ssjdoudou/article/details/103318019/">《Hexo-next主题支持数学公式》 by请叫我算术嘉</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>[Paper Review] MaxUp: Lightweight Adversarial Training with Data Augmentation Improves Neural Network Training</title>
    <url>/2021/07/16/MaxUp-Method/</url>
    <content><![CDATA[<div align="center"> <img src="/2021/07/16/MaxUp-Method/authors.jpg" width="650"></div>

<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Alleviate the issue of over-fitting and improve generalization performance on testing data:<br><a id="more"></a></p>
<ul>
<li>Regularization-based methods – introduce penalty on the complexity of the model.</li>
<li>Data augmentation techniques – leverage important invariance properties of the data.</li>
</ul>
<h1 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Idea: Generate a set of augmented data with some random perturbations or transforms, and minimize the maximum or worst case loss over the augmented data.</p>
<ul>
<li>Given a dataset $D_n=\{x_i\}_{i=1}^n$, for each point x in $D_n$, generate a set of perturbed data points $\{x_i’\}_{i=1}^m$</li>
<li><p>Estimate $\theta$ by minimizing the maximum loss over $\{x’_i\}$</p>
<div align="center"> <img src="/2021/07/16/MaxUp-Method/fig1.jpg" width="250"></div>

</li>
</ul>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><div align="center"> <img src="/2021/07/16/MaxUp-Method/alg.jpg" width="700"></div>

<div align="center"> <img src="/2021/07/16/MaxUp-Method/fig2.jpg" width="400"></div>

<p>Time&amp;Mem Cost: Only need to find the worst instance out of the <font color="#32adff">m</font> augmented copies through <font color="#32adff">forward propagation</font></p>
<h1 id="Theoretical-Interpretation"><a href="#Theoretical-Interpretation" class="headerlink" title="Theoretical Interpretation"></a>Theoretical Interpretation</h1><font color="#32adff">MaxUp introduces an extra gradient-norm regularization over standard data augmentation to encourage smoothness.</font>

<h2 id="Standard-data-augmentation"><a href="#Standard-data-augmentation" class="headerlink" title="Standard data augmentation:"></a>Standard data augmentation:</h2><p>Standard data augmentation minimizes the average loss: <font color="#ffaad4">$\mathop{min}\limits_\theta\mathbb{E}_{x\sim D_n}\left[\frac{1}{m}\sum\limits^m\limits_{i=1}L(x’_i,\theta)\right]$</font></p>
<p>Define the expected loss on data point $x$: <font color="#ffaad4">$\hat{L}_{\mathbb{P},m}(x, \theta):=\mathbb{E}_{\{x’_i\}^m_{i=1}\sim\mathbb{P}(\cdot|x)^m}\left[\frac{1}{m}\sum\limits_{i=1}\limits^mL(x’_i,\theta)\right]$</font></p>
<ul>
<li>$L(x, \theta)$ is second-order differentiable w.r.t. $x$.</li>
<li>$\mathbb{P}(\cdot|x)$ is any distribution whose expectation is $x$.</li>
</ul>
<p>With Taylor expansion: <font color="#ffaad4">$\hat{L}_{\mathbb{P},m}(x, \theta) = L(x, \theta) + O(\sigma^2)$</font></p>
<font color="32adff">

(The first-order term in the Taylor expansion is canceled out due to the averaging)</font>

<h2 id="MaxUp-Method"><a href="#MaxUp-Method" class="headerlink" title="MaxUp Method:"></a>MaxUp Method:</h2><p>Define:</p>
<div align="center"> <img src="/2021/07/16/MaxUp-Method/theo_def.jpg" width="300"></div>

<p>$L^{max}_{\mathbb{P},m}(x,\theta)$ and $L^{avg}_{\mathbb{P},m}(x,\theta)$ denote the expected MaxUp and typical average risk on data point $x$ with $m$ augmented copies respectively.</p>
<font color="32adff">Assume: $L(x,\theta)$ is second-order differentiable w.r.t. $x\in R^d$ and the variance of $\mathbb{P}(\cdot|x)$ is bounded by $\sigma^2$.</font>

<p>For every positive integer $m$,</p>
<div align="center"> <img src="/2021/07/16/MaxUp-Method/theo.jpg" width="400"></div>

<p>, where $c_{\mathbb{P},m}^+\ge c_{\mathbb{P},m}^-\ge 0$ are two non-negative coefficients.</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Image-classification"><a href="#Image-classification" class="headerlink" title="Image classification"></a>Image classification</h2><h3 id="CutMix-for-ImageNet"><a href="#CutMix-for-ImageNet" class="headerlink" title="CutMix for ImageNet"></a>CutMix for ImageNet</h3><ul>
<li>CutMix randomly cuts and pasts patches among training images, while the ground truth labels are also mixed proportionally to the area of the patches.</li>
<li>Only fine-tune the pretrained models with MaxUp for a few epochs.</li>
<li>$m = 4$</li>
</ul>
<div align="center"> <img src="/2021/07/16/MaxUp-Method/exp_cutmix.jpg" width="700"></div>

<h3 id="Cutout-for-CIFAR10-100"><a href="#Cutout-for-CIFAR10-100" class="headerlink" title="Cutout for CIFAR10/100"></a>Cutout for CIFAR10/100</h3><div align="center"> <img src="/2021/07/16/MaxUp-Method/exp_cifar10_cutout.jpg" width="450"></div>

<div align="center"> <img src="/2021/07/16/MaxUp-Method/exp_cifar100_cutout.jpg" width="450"></div>

<div align="center"> <img src="/2021/07/16/MaxUp-Method/exp_m_cutout.jpg" width="450"></div>

<h2 id="Adversarial-certification"><a href="#Adversarial-certification" class="headerlink" title="Adversarial certification"></a>Adversarial certification</h2><p>Adversarial Defense:</p>
<ul>
<li>Empirical Defense: Empirically seem robust to known adversarial attacks.</li>
<li><font color="#32adff">Certified Defense: Provably robust to certain kinds of adversarial perturbations</font>

</li>
</ul>
<p>MaxUp can be viewed as a “lightweight” variant of adversarial training against adversarial input perturbations <font color="32adff">(optimize over the worst samples)</font>.</p>
<p>Use <strong>Gaussian Perturbation</strong> as the data augmentation method:</p>
<div align="center"> <img src="/2021/07/16/MaxUp-Method/exp_adv.jpg" width="700"></div>

<p>[4] “[2019-ICML] Certified Adversarial Robustness via Randomized Smoothing”: Trains the classifier with a Gaussian data augmentation.</p>
<p>[18] “[2019-NeurIPS] Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers”: Improves the original Gaussian data augmentation by using PGD adversarial training (PGD is used to find a local maximal within a given $l_2$ perturbation ball).</p>
<h3 id="Evaluation-Method"><a href="#Evaluation-Method" class="headerlink" title="Evaluation Method"></a>Evaluation Method</h3><p>Certified accuracy: the fraction of the test set which <font color="#32adff"><strong>CERTIFY</strong> classifies correctly</font> and <font color="#32adff">certifies robust with a radius $R \le r$</font>.</p>
<ul>
<li>$f$: classifier</li>
<li>$n_0$: a same number of samples to take a guess</li>
<li>$n$: a larger number of samples to estimate $\underline{p_A}$</li>
<li>$\alpha$: failure probability</li>
</ul>
<div align="center"> <img src="/2021/07/16/MaxUp-Method/alg_cert.jpg" width="450"></div>

<h2 id="Accelerate-Training"><a href="#Accelerate-Training" class="headerlink" title="Accelerate Training"></a>Accelerate Training</h2><p>For image related tasks: use low-resolution images when selecting the worst case among augmented images.</p>
<div align="center"> <img src="/2021/07/16/MaxUp-Method/exp_acc.jpg" width="700"></div>

<p>For other kinds of data type, e.g. point cloud, video, we can also use sub-sampled particles, sub-sampled spatial-temporal pairs.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>MaxUp Idea: generate a set of augmented data, and <font color="#32adff">minimize the maximum or worst case loss over the augmented data</font>.</p>
<ul>
<li>Improve the generation performance (image classification, 3D point cloud classification).</li>
<li>Increase the adversarial robustness in Gaussian adversarial certification.</li>
<li>Possible ways to accelerate training (e.g. low-resolution for image classification).</li>
</ul>
<p>Resource: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Gong_MaxUp_Lightweight_Adversarial_Training_With_Data_Augmentation_Improves_Neural_Network_CVPR_2021_paper.pdf">MaxUp: Lightweight Adversarial Training with Data Augmentation Improves Neural Network Training</a></p>
]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Adversarial Training</tag>
        <tag>Improve Generalization</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR2021 Papers About Adversarial Network</title>
    <url>/2021/07/14/CVPR2021-Papers/</url>
    <content><![CDATA[<h1 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training:"></a>Adversarial Training:</h1><h2 id="MaxUp-Lightweight-Adversarial-Training-with-Data-Augmentation-Improves-Neural-Network-Training"><a href="#MaxUp-Lightweight-Adversarial-Training-with-Data-Augmentation-Improves-Neural-Network-Training" class="headerlink" title="MaxUp: Lightweight Adversarial Training with Data Augmentation Improves Neural Network Training"></a><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Gong_MaxUp_Lightweight_Adversarial_Training_With_Data_Augmentation_Improves_Neural_Network_CVPR_2021_paper.pdf">MaxUp: Lightweight Adversarial Training with Data Augmentation Improves Neural Network Training</a></h2><p><strong>Main Idea</strong>: Generate a set of augmented samples, then <font color="#32adff">optimize over the worst (or highest) loss image</font> —&gt; Improve the model generalization.<br><a id="more"></a></p>
<p><strong>Approach</strong>: Do forward propagation for $m$ augmented images of each image and find the one with highest(worst) loss. Then calculate the ERM based on the worst images in a mini-batch.</p>
<div align="center"> <img src="/2021/07/14/CVPR2021-Papers/maxup_al.jpg" width="700"></div>

<p>For more details about this paper, please see <a href="/2021/07/16/MaxUp-Method/" title="[Paper Review] MaxUp: Lightweight Adversarial Training with Data Augmentation Improves Neural Network Training">[Paper Review] MaxUp: Lightweight Adversarial Training with Data Augmentation Improves Neural Network Training</a>.</p>
<h2 id="Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation"><a href="#Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation" class="headerlink" title="Regularizing Neural Networks via Adversarial Model Perturbation"></a><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Regularizing_Neural_Networks_via_Adversarial_Model_Perturbation_CVPR_2021_paper.pdf">Regularizing Neural Networks via Adversarial Model Perturbation</a></h2><p><strong>Problem</strong>: ERM training is prone to overfitting (lack of generalization ability).</p>
<p><strong>Aim</strong>: Provide an effective regularization technique and take usage of flat minima. —&gt; <font color="#32adff">Improve the model’s generalization capability</font>.</p>
<p><strong>Main idea</strong>:<br>Instead of directly minimizing the empirical risk, an alternative “AMP loss” is minimized via SGD. Specifically, the AMP loss is obtained from the empirical risk by <font color="#32adff">applying the “worst” norm-bounded perturbation <strong>highest loss</strong> on each point in the parameter space</font>.</p>
<div align="center"> <img src="/2021/07/14/CVPR2021-Papers/amp_figure1.jpg" width="350"></div>

<p><strong>Approach</strong>:</p>
<div align="center"> <img src="/2021/07/14/CVPR2021-Papers/amp_algorithm.jpg" width="350"></div>

<ul>
<li><p>AMP loss: Do adversarial perturbation for $\theta$ to find the $\theta + \Delta_B$ with the worst (or highest) ER within $\epsilon$.</p>
<div align="center"> <img src="/2021/07/14/CVPR2021-Papers/amp_figure2.jpg" width="260"></div>
</li>
<li><p>Then minimize the AMP loss via SGD.</p>
</li>
</ul>
<p>For more details about this paper, please see <a href="/2021/07/20/AMP-Regularizing-Neural-Networks-via-Adversarial-Model-Perturbation/" title="[Paper-Review] Regularizing Neural Networks via Adversarial Model Perturbation">[Paper-Review] Regularizing Neural Networks via Adversarial Model Perturbation</a>.</p>
<h2 id="Robust-and-Accurate-Object-Detection-via-Adversarial-Learning"><a href="#Robust-and-Accurate-Object-Detection-via-Adversarial-Learning" class="headerlink" title="Robust and Accurate Object Detection via Adversarial Learning"></a><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Robust_and_Accurate_Object_Detection_via_Adversarial_Learning_CVPR_2021_paper.pdf">Robust and Accurate Object Detection via Adversarial Learning</a></h2><p><strong>Main Idea</strong>: This work instead <font color="#32adff">augments the fine-tuning stage for object detectors by exploring adversarial examples</font>, which can be viewed as a model-dependent data augmentation. Our method dynamically selects the stronger adversarial images sourced from a detector’s classification and localization branches and evolves with the detector to ensure the augmentation policy stays current and relevant.</p>
<p><strong>Approach</strong>:<br>Generate the adversarial $x^i_{cls}$ and $x^i_{loc}$ using FGSM. Then find the one maximization the  total loss. <font color="#32adff">(Just utilize the adversarial training method for object detection.)</font></p>
<div align="center"> <img src="/2021/07/14/CVPR2021-Papers/detection_al.jpg" width="400"></div>

<h1 id="Physical-Adversarial-Attack"><a href="#Physical-Adversarial-Attack" class="headerlink" title="Physical Adversarial Attack:"></a>Physical Adversarial Attack:</h1><h2 id="Adversarial-Imaging-Pipelines"><a href="#Adversarial-Imaging-Pipelines" class="headerlink" title="Adversarial Imaging Pipelines"></a><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Phan_Adversarial_Imaging_Pipelines_CVPR_2021_paper.pdf">Adversarial Imaging Pipelines</a></h2><p><strong>Main Idea</strong>: Existing attack methods aim to deceive CNN-based classifiers by manipulating RGB images that are fed directly to the classifiers. (Neglect <font color="#32adff">the influence of the camera optics and image processing pipeline</font> that produce the network inputs)</p>
<p><strong>Approach</strong>:</p>
<div align="center"> <img src="/2021/07/14/CVPR2021-Papers/pipeline_fig.jpg"></div>

<h2 id="Invisible-Perturbations-Physical-Adversarial-Examples-Exploiting-the-Rolling-Shutter-Effect"><a href="#Invisible-Perturbations-Physical-Adversarial-Examples-Exploiting-the-Rolling-Shutter-Effect" class="headerlink" title="Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect"></a><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Sayles_Invisible_Perturbations_Physical_Adversarial_Examples_Exploiting_the_Rolling_Shutter_Effect_CVPR_2021_paper.pdf">Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect</a></h2><p><strong>Main Idea</strong>: <font color="#32adff">Modify light data that illuminates the object</font> to generate physical adversarial examples that are <font color="#32adff">invisible to human eyes</font>.</p>
<p><strong>Approach</strong>:</p>
<ul>
<li>The attacker light source<br>flickers at a <font color="#32adff">frequency that humans cannot perceive</font>, and thus,<br>the scene simply appears to be illuminated.</li>
<li>Compute a light signal $f(t)$ such that, when an image is taken under the influence of this light signal, the <font color="#32adff">loss is minimized between the model output and the desired target class</font>.</li>
</ul>
<div align="center"> <img src="/2021/07/14/CVPR2021-Papers/invisible_al.jpg" width="450"></div>


<h2 id="Adversarial-Laser-Beam-Effective-Physical-World-Attack-to-DNNs-in-a-Blink"><a href="#Adversarial-Laser-Beam-Effective-Physical-World-Attack-to-DNNs-in-a-Blink" class="headerlink" title="Adversarial Laser Beam: Effective Physical-World Attack to DNNs in a Blink"></a><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Duan_Adversarial_Laser_Beam_Effective_Physical-World_Attack_to_DNNs_in_a_CVPR_2021_paper.pdf">Adversarial Laser Beam: Effective Physical-World Attack to DNNs in a Blink</a></h2><p><strong>Main Idea</strong>: Adversarial <font color="#32adff">Laser Beam</font> —&gt; enable manipulation of laser beam’s physical parameters to perform adversarial attack.</p>
<div align="center"> <img src="/2021/07/14/CVPR2021-Papers/beam_example.jpg" width="300"></div>

<p><strong>Approach</strong>:</p>
<ul>
<li><p><font color="#32adff">A greedy search with k-random-restart</font> for a vector of physical parameters $\theta$ of laser beam $l$ given image $x$, aiming to result in misclassification by $f$.</p>
</li>
<li><p>Use <font color="#32adff">confidence score based on a target model</font> because an attacker cannot attain the knowledge of the target model.</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Adversarial Network</category>
      </categories>
      <tags>
        <tag>Adversarial Training</tag>
        <tag>CVPR</tag>
      </tags>
  </entry>
  <entry>
    <title>Adversarial Attack and Defense</title>
    <url>/2021/07/13/Attack/</url>
    <content><![CDATA[<h1 id="Adversarial-Attacks"><a href="#Adversarial-Attacks" class="headerlink" title="Adversarial Attacks"></a>Adversarial Attacks</h1><a id="more"></a>
<h1 id="Adversarial-Defense"><a href="#Adversarial-Defense" class="headerlink" title="Adversarial Defense"></a>Adversarial Defense</h1><h2 id="Empirical-Defense"><a href="#Empirical-Defense" class="headerlink" title="Empirical Defense"></a>Empirical Defense</h2><p>Empirically seem robust to known adversarial attacks.</p>
<h3 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training"></a>Adversarial Training</h3><p>Adversarial examples are found during training (often using projected gradient descent) and added to the training set.</p>
<h2 id="Certified-Defense"><a href="#Certified-Defense" class="headerlink" title="Certified Defense"></a>Certified Defense</h2><p>Provably robust to certain kinds of adversarial perturbations.</p>
<p>For any input x,  one can easily obtain a guarantee that the classifier’s prediction is constant within some set around x, $l_2$ or $l_/infinity$ ball.</p>
<ul>
<li>Certified test set accuracy at radius r: the fraction of the test set which g classifies correctly with a prediction that is certifiably robust within an $l_2$ ball of radius r.</li>
<li>Approximate ~: the fraction of the test set which <strong>CERTIFY</strong> classifies correctly and certifies robust with a radius $R \le r$</li>
</ul>
]]></content>
      <tags>
        <tag>Overview</tag>
      </tags>
  </entry>
  <entry>
    <title>OCR-Data</title>
    <url>/2021/05/09/OCR-Data/</url>
    <content><![CDATA[<p>This blog talks about the popular used <strong>Text Recognition Data</strong></p>
<h2 id="Synthetic-Word-Dataset"><a href="#Synthetic-Word-Dataset" class="headerlink" title="Synthetic Word Dataset"></a>Synthetic Word Dataset</h2><p><a href="https://www.robots.ox.ac.uk/~vgg/data/text/">https://www.robots.ox.ac.uk/~vgg/data/text/</a><br><a id="more"></a><br>This is synthetically generated dataset which we found sufficient for training text recognition on real-world images.</p>
<p><img src="/2021/05/09/OCR-Data/Synth90k.jpg" alt="figure1" title="Synthetic 90k Example"></p>
<p>Use linux remote server to download the dataset:<br><code>wget https://thor.robots.ox.ac.uk/~vgg/data/text/mjsynth.tar.gz</code></p>
<h1 id><a href="#" class="headerlink" title="#"></a>#</h1>]]></content>
      <categories>
        <category>Scene Text Recognition</category>
      </categories>
      <tags>
        <tag>STR</tag>
      </tags>
  </entry>
  <entry>
    <title>FGSM Attack</title>
    <url>/2021/03/08/FGSM-Attack/</url>
    <content><![CDATA[<p>Resource: <a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a></p>
]]></content>
      <categories>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Training</tag>
        <tag>Attack</tag>
        <tag>hide</tag>
      </tags>
  </entry>
  <entry>
    <title>AGD Method (Adversarial Example Detection)</title>
    <url>/2021/02/03/AGD-Method-Adversarial-Example-Detection/</url>
    <content><![CDATA[<p>Resource: <a href="https://arxiv.org/pdf/1507.05717.pdf">2021 AAAI-Beating Attackers At Their Own Games:<br>Adversarial Example Detection Using Adversarial Gradient Directions</a></p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Previous adversarial example detection methods primarily exploit the following two observed properties of adversarial examples:<br><a id="more"></a></p>
<ul>
<li>Adversarial examples are comparatively more sensitive to perturbations in the input space than benign examples.</li>
<li>The distance of an adversarial example to the data distribution of benign examples is anomalous.</li>
</ul>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p><img src="/2021/02/03/AGD-Method-Adversarial-Example-Detection/Approach_overview.jpg" alt="figure1" title="Approach overview"></p>
<h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><ul>
<li>$I^q$: query Example</li>
<li>$\phi(f(I), y)$: the loss function for the classifier f</li>
<li>$I’$: adversarial example</li>
<li>$$:</li>
<li>$\tau(I^q) \rightarrow [0,1]$: adversarial example detector</li>
<li>$I^p_l = I^q * T_l$: A transformed image</li>
<li>$T_l$: an image transformation</li>
<li>$I^n$: a prototype benign example that belongs to the predicted class a from a reference database $D’$</li>
</ul>
<h3 id="Scores"><a href="#Scores" class="headerlink" title="Scores"></a>Scores</h3><ul>
<li>$\alpha_a$: compute the angular similarity between $I^q$ and $I^p$ —&gt; query and transformed</li>
<li>$\beta_k$: compute the angular similarity between $I^q$ and $I^n$ — query and benign</li>
<li>$\gamma_k$: compute the angular similarity between $I^p$ and $I^n$ — transformed and benign</li>
</ul>
]]></content>
      <categories>
        <category>Adversarial Detection</category>
      </categories>
      <tags>
        <tag>Adversarial</tag>
        <tag>Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA Method</title>
    <url>/2021/01/27/PCA-Method/</url>
    <content><![CDATA[<p>Resource: <a href="https://arxiv.org/pdf/1608.00530.pdf">EARLY METHODS FOR DETECTING ADVERSARIAL IMAGES</a></p>
<p>Code: <a href="https://github.com/hendrycks/fooling">https://github.com/hendrycks/fooling</a></p>
<h2 id="Principle"><a href="#Principle" class="headerlink" title="Principle"></a>Principle</h2><p>Detect the adversarial images using the difference between coefficients of low-ranked principal components.</p>
<a id="more"></a>
<p><img src="/2021/01/27/PCA-Method/diff_of_coefficients.jpg" alt="figure1" title="Differences of Low-Ranked Principle Components"></p>
<p>As is shown in the above figure, if we use the original, the adversarial images will have different coefficients for the low-ranked principal components than do clean images.</p>
<p>Thus, we can use the <strong>coefficient variance</strong> to detect the adversarial examples.</p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>AUROC: Area Under the Receiver Operating Characteristic (AU-ROC) metric</p>
<ul>
<li>ROC curve shows the true positive (tpr = tp/(tp+fn)) and false positive  (fpr = fp/(fp+tn)) rate against each other.</li>
</ul>
<p>AUPR: Area Under the Precision Recall curve</p>
<ul>
<li>PR curve plots the precision (tp/(tp+fp)) and recall (tp/tp+fn) against each other.</li>
</ul>
<h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><p>For this paper, they test the PCA method on Tiny-ImageNet, CIFAR-10 and MNIST. (They do not split the dataset into train and test dataset.)</p>
<p><img src="/2021/01/27/PCA-Method/Performance.jpg" alt="figure1" title="Performance"></p>
<h2 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h2><ul>
<li><p>If we intend to reach high scores of both AUROC and AUPR, the training dataset (the samples used to generate the PCA) and the testing dataset (clean ones) should be similar to each other.</p>
<p><strong>Note:</strong> The similarity is only meaningful to the magical program of generating PCA.</p>
</li>
<li><p>Only the AUROC and AUPR are not enough to finish the detection, we need to provide a kind of threshold <strong>(maybe the binary search is a possible method)</strong>.</p>
</li>
</ul>
<h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><p>Use the PCA of adversarial examples of the training dataset to do auxiliary detection.</p>
]]></content>
      <categories>
        <category>Adversarial Detection</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN &amp; LSTM</title>
    <url>/2021/01/15/RNN-LSTM/</url>
    <content><![CDATA[<p>Resource:</p>
<ul>
<li><a href="https://www.deeplearningbook.org/contents/rnn.html">RNN Chapter of the DeepLearningBook</a></li>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks, colah’s blog</a></li>
</ul>
<p>In this passage, I will introduce the basic knowledge of RNN (Recurrent Neural Network) and its widely used type LSTM (Long Short Term Memory).</p>
<h2 id="RNN-Network"><a href="#RNN-Network" class="headerlink" title="RNN Network"></a>RNN Network</h2><p>Recurrent neural networks are specialized for processing a sequence of values <strong>$x^{(1)}, …, x^{(T)}$</strong> (input).<br><a id="more"></a></p>
<p><img src="/2021/01/15/RNN-LSTM/RNN_structure.jpg" alt="RNN_structure" title="The Structure of RNN"></p>
<p>Above figure shows the basic structure of RNN.</p>
<ul>
<li><strong>$o$</strong> is the output values</li>
<li>$L$ measures how for each <strong>$o$</strong> is from the corresponding training target <strong>$y$</strong>.</li>
</ul>
<p>The for each $t$ from $1$ to $T$, we can apply the below formulations.</p>
<p><img src="/2021/01/15/RNN-LSTM/RNN_formula.jpg" alt="RNN_formula" title="Example formulations of RNN"></p>
<p>Then, for each input $x_t (t \in {1,2,…,T})$ with length $n_i$ (a column vector), we assume that</p>
<script type="math/tex; mode=display">x_t = [x_{t,1}, x_{t,2}, x_{t,3}, ..., x_{t, n_i}]^T</script><p>And the hidden layer component $h_t$ with length $n_h$</p>
<script type="math/tex; mode=display">h_t = [h_{t,1}, h_{t,2}, h_{t,3}, ..., h_{t, n_h}]^T</script><p>The output $y_t$ with length $n_o$</p>
<script type="math/tex; mode=display">y_t = [y_{t,1}, y_{t,2}, y_{t,3}, ..., y_{t, n_o}]^T</script><p><img src="/2021/01/15/RNN-LSTM/RNN_structure_singlelayer.jpg" alt="RNN_layer" title="Details of Single Frame for RNN"></p>
<p><strong>Note: Sharing Variables</strong> For an arbitrary $t \in {1,2,…,T}$, all the weights <strong>$U, W, V, B$</strong> are the same.</p>
<h3 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h3><p>Sometimes, we also need the further contextual words to predict the $y_t$.</p>
<p><img src="/2021/01/15/RNN-LSTM/BiRNN_structure.jpg" alt="RNN_bi" title="Structure of Bidirectional RNN"></p>
<p>Above figure shows the basic structure of Bidirectional RNN.</p>
<ul>
<li><strong>$h^{(t)}$</strong> standing for the state of the sub-RNN that moves forward through time</li>
<li><strong>$g^{(t)}$</strong> standing for the state of the sub-RNN that moves backward through time</li>
</ul>
<h2 id="LSTM-Network"><a href="#LSTM-Network" class="headerlink" title="LSTM Network"></a>LSTM Network</h2><h3 id="Drawback-of-RNN"><a href="#Drawback-of-RNN" class="headerlink" title="Drawback of RNN"></a>Drawback of RNN</h3><p>RNN network determines the output <strong>$y_t$</strong> based on its previous context information, but sometimes</p>
<ul>
<li>Prediction does not need any further context.<ul>
<li>E.g. predict the last word in “the clouds are in the <em>sky</em>“</li>
</ul>
</li>
<li>Prediction needs contextual words from further back.</li>
</ul>
<h3 id="LSTM-Structure"><a href="#LSTM-Structure" class="headerlink" title="LSTM Structure"></a>LSTM Structure</h3><p>The overall structure of LSTM is the same with basic RNN structure, but it has a different repeating cell, as is shown in below pictures.</p>
<p><img src="/2021/01/15/RNN-LSTM/LSTM_blockdiagram.jpg" alt="LSTM_diagram" title="Block Diagram of LSTM Cell"></p>
<p><img src="/2021/01/15/RNN-LSTM/LSTM_structure1.jpg" alt="LSTM_diagram2" title="Block Diagram of LSTM Cell"></p>
<p>For the above diagram, the notations respectively mean that:</p>
<p><img src="/2021/01/15/RNN-LSTM/LSTM_notation.jpg" alt="LSTM_notation" title="Notation of LSTM Diagram"></p>
<p>The whole idea of the LSTM is to use <strong>sigmoid function</strong> to control what information we are going to use. And we need to determine the <strong>$C_t$</strong> and <strong>$h_t$</strong>.</p>
<ul>
<li><p><strong>$C_t$</strong> is determined by the forget and input gates.</p>
</li>
<li><p><strong>$h_t$</strong> is determined by the output gate.</p>
</li>
</ul>
<h4 id="Forget-Gate"><a href="#Forget-Gate" class="headerlink" title="Forget Gate"></a>Forget Gate</h4><p>For this forget gate, we can decide what information we are going to <strong>throw away</strong> from the <strong>cell state</strong> <strong>&amp;C_{t-1}&amp;</strong> y using a <strong>sigmoid layer</strong>.</p>
<ul>
<li>1 represents “completely keep this”</li>
<li>o represents “completely get rid of this”</li>
</ul>
<p><img src="/2021/01/15/RNN-LSTM/LSTM_Forget.jpg" alt="LSTM_Forget" title="Forget Gate of LSTM Diagram"></p>
<h4 id="Input-Gate"><a href="#Input-Gate" class="headerlink" title="Input Gate"></a>Input Gate</h4><p>For this gate, we can decide what information we are going to <strong>store</strong> in the <strong>cell state</strong>. This consists two parts.</p>
<ol>
<li>A <strong>sigmoid layer</strong> decides which values we will update.</li>
<li>A <strong>tanh layer</strong> creates a vector of new candidates values <strong>&amp;C’_t&amp;</strong>, that could be added to the state.</li>
</ol>
<p><img src="/2021/01/15/RNN-LSTM/LSTM_Input.jpg" alt="LSTM_Input" title="Input Gate of LSTM Diagram"></p>
<p>Then, we could calculate the cell state <strong>&amp;C_t&amp;</strong>, which is shown in the following figure.</p>
<p><img src="/2021/01/15/RNN-LSTM/LSTM_Ct.jpg" alt="LSTM_Ct" title="Calculation of Cell State $C_t$"></p>
<h4 id="Output-Gate"><a href="#Output-Gate" class="headerlink" title="Output Gate"></a>Output Gate</h4><p>In this gate, we decide what we are going to output. It needs the <strong>cell state</strong> <strong>$C_t$</strong>, current input <strong>$x_t$</strong> and the <strong>$h_{t-1}$</strong> just like the basic RNN.</p>
<p><img src="/2021/01/15/RNN-LSTM/LSTM_Output.jpg" alt="LSTM_Output" title="Output Gate of LSTM Diagram"></p>
<p><strong>Note:</strong> The difference between the output <strong>$h_t$</strong> of basic RNN and LSTM is the component <strong>cell state</strong> <strong>$C_t$</strong>.</p>
]]></content>
      <categories>
        <category>Basic Neural Networks</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>About SSH</title>
    <url>/2021/01/15/About-SSH/</url>
    <content><![CDATA[<h2 id="SSH-tool"><a href="#SSH-tool" class="headerlink" title="SSH tool"></a>SSH tool</h2><p>I use MobaXterm (<a href="https://mobaxterm.mobatek.net/">official website</a>), since it enables the visualization of many operations.</p>
<p>Select session -&gt; SSH; then type in the server ip, username and password.<br><a id="more"></a><br><img src="/2021/01/15/About-SSH/figure_SSH.jpg" alt="figureSSH"></p>
<h2 id="Pycharm-connection"><a href="#Pycharm-connection" class="headerlink" title="Pycharm connection"></a>Pycharm connection</h2><p>I omit the basic downloading and  installation here. You can just google Pycharm and download its <strong>Professional</strong> version from its official website.</p>
<p>Note: students and professors can use academic</p>
<ol>
<li><p>Select tools -&gt; Deployment -&gt; Configuration<br><img src="/2021/01/15/About-SSH/figure_1.jpg" alt="figure1"></p>
</li>
<li><p>Press “+” button and choose the SFTP (Secure File Transfer Protocol)<br><img src="/2021/01/15/About-SSH/figure_2.jpg" alt="figure2"></p>
</li>
<li><p>Add a new SSH configuration<br><img src="/2021/01/15/About-SSH/figure_3.jpg" alt="figure3"></p>
<p><img src="/2021/01/15/About-SSH/figure_4.jpg" alt="figure4"></p>
</li>
<li><p>Root path is the location of the remote server you want to transfer files to; then for <strong>Mappings</strong>, local path is your local project’s path.<br><img src="/2021/01/15/About-SSH/figure_5.jpg" alt="figure5"></p>
<p><img src="/2021/01/15/About-SSH/figure_6.jpg" alt="figure6"></p>
</li>
</ol>
]]></content>
      <categories>
        <category>Remote Server</category>
      </categories>
      <tags>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>CTC Method</title>
    <url>/2021/01/15/CTC-Method/</url>
    <content><![CDATA[<p>Resource:</p>
<ol>
<li><a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">Connectionist Temporal Classification: Labelling Unsegmented<br>Sequence Data with Recurrent Neural Networks</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/43534801">https://zhuanlan.zhihu.com/p/43534801</a></li>
<li><a href="https://distill.pub/2017/ctc/">Sequence Modeling With CTC</a></li>
</ol>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Traditional RNNs require <strong>pre-segmented</strong> training data and <strong>post-processing</strong> to transform their outputs into label sequence.</p>
<p>Thus, CTC intends to presents a novel method for training RNNs to label unsegmented sequences directly.<br><a id="more"></a></p>
<p>CTC needs to do two tasks efficiently:</p>
<ul>
<li><p><strong>Loss Function:</strong> For a given input, train our model to maximize the probability it assigns to the right master. <font color="32adff">Efficiently compute the conditional probability $p(Y|X)$, which should also be differentiable.</font></p>
</li>
<li><p><strong>Inference:</strong> Solve $Y^*=\mathop{argmax}\limits_Y\,p(Y|X)$. With CTC settle for an approximate solution that is not too expensive to find.</p>
</li>
</ul>
<h1 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h1><h2 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h2><div align="center"> <img src="/2021/01/15/CTC-Method/alignment.jpg" width="650"></div>

<ul>
<li>The allowed alignments between $X$ and $Y$ are monotonic.</li>
<li>The alignment of $X$ to $Y$ is many-to-one.</li>
<li>The length of $Y$ cannot be greater than the length of $X$.</li>
</ul>
<div align="center"> <img src="/2021/01/15/CTC-Method/alignment2.jpg" width="650"></div>

<h2 id="Path-Probability"><a href="#Path-Probability" class="headerlink" title="Path Probability"></a>Path Probability</h2><ul>
<li>Input: <strong>$x$</strong> of length <strong>$T$</strong>, each element in <strong>$x$</strong> has $m$ dimensions.<ul>
<li>$x = (x^1, x^2, x^3, …, x^T)$</li>
<li>$x^t = (x^t_1, x^t_2, x^t_3, …, x^t_m)$</li>
</ul>
</li>
<li>Outputs: <strong>$y$</strong> of length <strong>$T$</strong>, each element in <strong>$y$</strong> has $n$ dimensions.<ul>
<li>$y = (y^1, y^2, y^3, …, y^T)$</li>
<li>$y^t = (y^t_1, y^t_2, y^t_3, …, y^t_n)$</li>
</ul>
</li>
<li>RNN Map: $N_w$ : $(R^m)^T \rightarrow (R^n)^T$<ul>
<li>$y = N_w(x)$</li>
</ul>
</li>
<li>Alphabet: $L’ = L \bigcup \{blank\}$</li>
<li>Path: $\pi \in L’^T$ means the sequence of predicted labels</li>
</ul>
<p>Thus, $y_k^t$ represents the probability of observing label $k$ at time $t$</p>
<p>The probability of a certain path $\pi$ over input $x$ is,</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/probability.jpg" width="450"></div>

<p>Note: the network outputs at different times must be conditionally independent.</p>
<h2 id="Labelling-Probability"><a href="#Labelling-Probability" class="headerlink" title="Labelling Probability"></a>Labelling Probability</h2><ul>
<li><p>Path to Labellings Map: $B$ : $L’^T \rightarrow L^{\leq T}$</p>
<ul>
<li>$L^{\leq T}$ is the set of possible labellings</li>
<li><p>E.g.</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/labellings_map.jpg" width="450"></div>

</li>
</ul>
</li>
</ul>
<p>Thus, we can use $B$ to define the conditional probability of a given labelling $l \in L^{\leq T}$ as the sum of the probabilities of all the paths corresponding to it:</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/labellings_prob.jpg" width="450"></div>

<h1 id="Decoding-Method"><a href="#Decoding-Method" class="headerlink" title="Decoding Method"></a>Decoding Method</h1><p>We aim to find the most probable labelling for the input sequence, which is:</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/predict.jpg" width="450"></div>

<p>Note: Refer to the task of finding this labelling as <strong>decoding</strong>.</p>
<p>There are two approximate methods:</p>
<h2 id="Best-Path-Decoding"><a href="#Best-Path-Decoding" class="headerlink" title="Best Path Decoding"></a>Best Path Decoding</h2><p>Assumption: The most probable path will correspond to the most probable labelling:</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/best_path_decoding.jpg" width="450"></div>

<h2 id="Prefix-Search-Decoding"><a href="#Prefix-Search-Decoding" class="headerlink" title="Prefix Search Decoding"></a>Prefix Search Decoding</h2><div align="center"> <img src="/2021/01/15/CTC-Method/prefix.jpg" width="700"></div>

<p>This method is based on the <strong>forward-backward algorithm</strong>.</p>
<p>Define <strong>$l’$</strong> as the labelling <strong>$l$</strong> with added to the beginnings and the end and inserted between every pair of labels. E.g.</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/l_example.jpg" width="450"></div>

<p>Then, $|l’| = 2|l| + 1$, where $|l|$ is the maximum length of labelling. For the above example, $|l| = 5$.</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/example_path.jpg" width="450"></div>

<h3 id="Forward-Variable"><a href="#Forward-Variable" class="headerlink" title="Forward Variable"></a>Forward Variable</h3><p>For the labelling <strong>$l$</strong>, define forward variable $\alpha_t(s)$ to be the total probability of $l_{1:s}$ at time $t$, where<br>$l_{1:s}$ presents labelling’s first $s$ symbols</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/variable_alpha.jpg" width="450"></div>

<ul>
<li><p>Initialization:</p>
<p>Since the first label ($t = 1$) of the path could be blank ($b$) or the first symbol in $l (l_1)$,  we have the following rules:</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/alpha_init.jpg" width="450"></div>
</li>
<li><p>Recursion:</p>
<p>Consider a certain time $t$, if the label of the path is $l_k \in L$, then for the time $t-1$, the possible choices are $l_k$, $l_{k-1}$ and $blank$.</p>
<div align="center"> <img src="/2021/01/15/CTC-Method/recurse.jpg" width="450"></div>

</li>
</ul>
<h3 id="Backward-Variable"><a href="#Backward-Variable" class="headerlink" title="Backward Variable"></a>Backward Variable</h3><h1 id="Training-CTC"><a href="#Training-CTC" class="headerlink" title="Training CTC"></a>Training CTC</h1><div align="center"> <img src="/2021/01/15/CTC-Method/CTC_training.jpg" width="450"></div>

<p>CTC training process is using $\frac{\partial p(l|x)}{\partial w}$ to adjust the CTC network weights $w$, which maximize the $p(l|x)$ when the input is $\pi \in B^{-1}(l)$</p>
<h1 id="CTC-Programming-Interface"><a href="#CTC-Programming-Interface" class="headerlink" title="CTC Programming Interface"></a>CTC Programming Interface</h1><p>Tensorflow:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.nn.ctc_loss(</span><br><span class="line">  labels,</span><br><span class="line">  inputs,</span><br><span class="line">  sequence_length,</span><br><span class="line">  preprocess_collapse_repeated = <span class="literal">False</span>,</span><br><span class="line">  ctc_merge_repreated = <span class="literal">True</span>,</span><br><span class="line">  ignore_longer_outputs_than_inputs = <span class="literal">False</span>,</span><br><span class="line">  time_major = <span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><br>Pytorch:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.CTCLoss(blank=<span class="number">0</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, zero_infinity=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CTC is a method to compute loss, which can make training samples do not need to be aligned.</p>
]]></content>
      <categories>
        <category>Scene Text Recognition</category>
      </categories>
      <tags>
        <tag>CTC</tag>
      </tags>
  </entry>
  <entry>
    <title>Extra Knowledge of Tensorflow</title>
    <url>/2021/01/14/Extra-Knowledge-of-Tensorflow/</url>
    <content><![CDATA[<h3 id="NCHW-amp-NHWC"><a href="#NCHW-amp-NHWC" class="headerlink" title="NCHW &amp; NHWC"></a>NCHW &amp; NHWC</h3><p>When we use the Tensorflow Application,<br>E.g.</p>
<p><code>tf.nn.conv2d(input, filter, strides, padding,
use_cudnn_on_gpu=None, data_formt = None, name=None)</code></p>
<p>data_format: default set to “NHWC”, which <strong>stipulate the orientation</strong> of input and output Tensor<br><a id="more"></a></p>
<ul>
<li>NHWC: [batch, height, width, channels]</li>
<li>NCHW: [batch, channels, height, width]</li>
</ul>
<p><img src="/2021/01/14/Extra-Knowledge-of-Tensorflow/Data_Format_Ex.jpg" alt="figure1" title="Example of Different Format"></p>
<p>Difference:</p>
<ul>
<li>The <strong>locality</strong> of access is <strong>better for NHWC</strong>.</li>
<li><strong>NCHW</strong> has to wait for all the channels to prepare the final output, which <strong>requires bigger temporary space</strong></li>
<li>NCHW is better for GPU, while NHWC better for CPU (most of situations)</li>
</ul>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Basic Knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title>CRNN Network</title>
    <url>/2021/01/13/CRNN-Network/</url>
    <content><![CDATA[<p>Resource: <a href="https://arxiv.org/pdf/1507.05717.pdf">An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</a></p>
<p>Implementation:</p>
<ul>
<li><a href="https://github.com/MaybeShewill-CV/CRNN_Tensorflow">Tensorflow, Author: MaybeShewill-CV</a></li>
<li><a href="https://github.com/meijieru/crnn.pytorch">Pytorch, Author: meijieru</a></li>
</ul>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>For image based sequence recognition, sequence-like objects, we need the system to predict <strong>a series of object labels</strong>.<br><a id="more"></a></p>
<ul>
<li><strong>DCNN models</strong> often operate on inputs and outputs with fixed dimensions —&gt; cannot be directly used.</li>
<li><strong>RNN models</strong> need a preprocessing step that convert an input image into a sequence of image features —&gt; cannot be trained and optimized in an end-to-end fashion.</li>
</ul>
<p>==&gt; <strong>Idea : merge</strong> the DCNN model and RNN model <strong>together</strong>, and try taking advantages of both these two models.</p>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>Convolutional Recurrent Neural Network &lt;— A combination of DCNN and RNN</p>
<p><img src="/images/CRNN_structure.jpg" alt="figure1" title="Structure of CRNN Network"></p>
<h3 id="Convolutional-Layers"><a href="#Convolutional-Layers" class="headerlink" title="Convolutional Layers"></a>Convolutional Layers</h3><ul>
<li>Structure: <strong>convolutional and max-pooling layers</strong> from a standard CNN model (fully-connected layers are removed)</li>
<li>Function: extract <strong>a sequential feature</strong> representation</li>
<li>Preprocess: scale all the images to the same height</li>
<li>Process:<ul>
<li>The _i_-th feature vector is the concatenation of the _i_-th columns of all maps;</li>
<li>The <strong><em>width</em></strong> of each column is fixed to <strong>single pixel</strong>;</li>
<li>Each column of the feature maps corresponds to a rectangle region of the original image</li>
</ul>
</li>
</ul>
<p><img src="/images/CRNN_Receptive_Field.jpg" alt="figure2" title="Structure of CRNN Network"></p>
<p>Code Example:</p>
<ul>
<li>input_size: [batch, 32, 100, 3] (NHWC format)</li>
<li>ouput_size: [batch, 1, 25, 512] (split the input image to 25 pitches from left to right)</li>
</ul>
<h3 id="Recurrent-Layers"><a href="#Recurrent-Layers" class="headerlink" title="Recurrent Layers"></a>Recurrent Layers</h3><ul>
<li><p>Structure: <strong>Deep bidirectional LSTM</strong></p>
<p><img src="/images/CRNN_RNN.jpg" alt="figure3" title="Structure of LSTM and Recurrent Layers"></p>
<ul>
<li>For basic knowledge of RNN network, please see <a href="/2021/01/15/RNN-LSTM/" title="RNN &amp; LSTM">RNN &amp; LSTM</a>.</li>
<li>LSTM (long-short Term Memory): a type of RNN unit to solve the vanishing gradient problem, whose structure shown in the above figure(a). For more information, also check the passage <a href="/2021/01/15/RNN-LSTM/" title="RNN &amp; LSTM">RNN &amp; LSTM</a>.</li>
<li>The recurrent layers first combine two LSTMs (one forward and one backward), then stack multiple bidirectional LSTMs (figure(b)).</li>
</ul>
</li>
<li><p>Function: predict a label distribution $y_t$ for each frame $x_t$ in the feature sequence $X = x_1,…,x_T$</p>
</li>
<li><p>Advantages:</p>
<ul>
<li>strong capability of capturing contextual information —&gt; better performance</li>
<li>can back-propagates error differentials to its input —&gt; jointly train CNN and RNN layers</li>
<li>can operate on sequences of arbitrary lengths</li>
</ul>
</li>
</ul>
<h3 id="Transcription"><a href="#Transcription" class="headerlink" title="Transcription"></a>Transcription</h3><p>Find the label sequence with the highest probability conditioned on the per-frame.</p>
<p>Take usage of Connectionist Temporal Classification (CTC), defining the conditional probability as the sum of probabilities of all $\pi$ that are mapped by $B$ onto $l$:</p>
<p><img src="/images/CNN_probability.jpg" alt="figure4" title="Contional Probability"></p>
<ul>
<li><p>$l$ is label sequence; $y = y_1, …, y_T$ are per-frame predictions; $T$ is the sequence length; $L’ = L \bigcup \{blank\}$, where $L$ contains all labels in the task; the path $\pi \in L’^T$; $B$ maps $\pi$ onto $l$</p>
</li>
<li><p>E.g. $B$ maps “—h-e-l-ll-oo—“ onto $hello$</p>
</li>
<li><p>The probability of $\pi$ is defined as $p(\pi | y) = \prod^T_{t=1}y^t_{\pi_t}$, where $y^t_{\pi_t}$ is the probability of having label $\pi_t$ at time stamp $t$.</p>
</li>
<li><p>CTC ignores the position where each label in $l$ is located, avoiding the labor of labeling positions of individual characters.</p>
</li>
<li><p>CTC utilizes forward-backward algorithm to calculate the conditional probability.</p>
</li>
<li><p>For details of CTC method, please see <a href="/2021/01/15/CTC-Method/" title="CTC Method">CTC Method</a>.</p>
</li>
</ul>
<h4 id="Lexicon-Free"><a href="#Lexicon-Free" class="headerlink" title="Lexicon-Free"></a>Lexicon-Free</h4><p>Use prefix search decoding (forward-backward) algorithm adopted in CTC to find the sequence $l^*$:</p>
<script type="math/tex; mode=display">l^* \approx B(argmax_\pi p(\pi | y))</script><h4 id="Lexicon-Based"><a href="#Lexicon-Based" class="headerlink" title="Lexicon-Based"></a>Lexicon-Based</h4><p>For lexicon $D$, we need to find $l^*$:</p>
<script type="math/tex; mode=display">l^* = argmax_{l \in D}p(l | y)</script><p>For large lexicons, performing an exhaustive search is very time-consuming.</p>
<p>Use lexicon-free transcription as baseline, and limit the search to the nearest-neighbor candidates $N_{\delta}(l’)$</p>
<p><img src="/images/CRNN_lexicon.jpg" alt="figure5" title="Lexicon Based Mode"></p>
<ul>
<li>$\delta$ is the maximal edit distance</li>
<li>$l’$ is the sequence transcribed from $y$ in lexicon-free mode</li>
<li>$N_{\delta}(l’)$ can be found efficiently with BK-tree in time $O(log|D|)$, where $|D|$ is the lexicon size.</li>
</ul>
<h2 id="Network-Training"><a href="#Network-Training" class="headerlink" title="Network Training"></a>Network Training</h2><h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><p>-</p>
]]></content>
      <categories>
        <category>Scene Text Recognition</category>
      </categories>
      <tags>
        <tag>STR</tag>
        <tag>CTC</tag>
      </tags>
  </entry>
</search>
